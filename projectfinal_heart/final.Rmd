---
title: "CUNY SPS DATA 621 - CTG5 - Final"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "May 23rd, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 5
        fig_height: 4
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
source("./source/libs.R")
source("./source/script.R")
source("./source/lr_betsy_gabby.R")
source("./source/rf_rose.R")
source("./source/captioner.R")
#set.seed(123)
```


\newpage

# PROJECT DESCRIPTION AND BACKGROUND

## Background

With nearly 18MM deaths in 2015  , cardiovascular diseases (CVD) are the leading cause of death globally and growing in the developing world .  CVD is a disease class which includes heart attacks, strokes, heart failure, coronary artery disease, arrythmia, venous thrombosis, and other conditions.  About half of all Americans (47%)  have at least one of three key risk factors for heart disease: high blood pressure, high cholesterol, and smoking.

Researchers estimate that up to 90% of heart disease deaths  could be prevented.  Typical means of detection include electrocardiograms (ECGs), stress tests, and cardiac angiograms, all of which are expensive.  Risk evaluation screenings require blood samples, which are assessed alongside risk factors like tobacco use, diet, sleep disorders, physical inactivity, air pollution, and others.

More efficient, scalable, and non-invasive means of early detection could be used to trigger medical interventions, prompt preventive care by physicians, and/or engender behavioral change on the part of those prone to or suffering from CVD.  Applying data mining techniques to CVD datasets to predict risk based on existing or easy-to-collect health data could improve healthcare outcomes and mortality rates.  

The Cleveland dataset is the most complete CVD dataset and is the most frequently used in data science experimentation.  It has a small number of cases (n = 303) but numerous variables (m = 14, including the target class) but this is not uncommon given the costs of experimental data and privacy risk of observational data.  

## Hypothesis

[Jeremy's inserting some thoughts here]

## Our approach, setup, and workflow

[We should discuss this]

\newpage

# DATA PREPARATION

## Description

[Someone to add in some descriptive stats]

```{r}
knitr::kable(vars, caption="Data Dictionary")
```

## Cross-validation

Cross Validation is a resampling procedure to evaluate machine learning model.  This approach involves randomly dividing the in-sample data into k groups or folds, of equal size.  The first fold is treated as a validation set and the method is fit on the remaining k-1 folds [22].  To use nested cross-validation, the outer cross-validation provides performance assessment obtainable using a method of constructing a model, including feature selection.  The inner cross-validation is used to select the features independently in each fold of the outer cross-validation [23].  We retain the evaluation score and discard the model to summarize the skill of the model using the evaluation scores.

One of the most popular cross validation techniques is Grid Search.  Grid search experiments are common in the literature of empirical machine learning, where they are used to optimize the hyper-parameters of learning algorithms.  It is common to perform multi-stage automated grid experiment, however, fine resolution for optimization would be computationally expensive.  Grid search experiments allocates many trials to the exploration of dimensions that may not matter and suffer from poor coverage in dimensions that are important.

On the other hand, Random search found better models in most cases and required less computational time.  It is easier to carry out and more practical in terms of statistical independence of every trial.  The experiment can be stopped at any time and can be added without adjustment of grid or committing larger experiment as every trial can be carried out asynchronously.

To minimize the computation time, we implement Coarse to Fine strategy.  During the Coarse search phase, we use the random search technique, then, simply filter the under-performing parameter values out to run Finer search to find the best values for parameters. 

## Bootstrap surrogata data

[Jeremy's editing this]

Per the synthpop packace explanation (https://cran.r-project.org/web/packages/synthpop/vignettes/synthpop.pdf):
"The basic idea of synthetic data is tovreplace some or all of the observed values by sampling from appropriate probability distributions so that the essential statistical features of the original data are preserved. The approach has been developed along similar lines to recent practical experience with multiple imputation methods although synthesis is not the same as imputation. Imputation replaces data which are missing with modelled values and adjusts the inference for the additional uncertainty due to this process. For synthesis, in the circumstances when some data are missing two approaches are possible, one being to impute missing values prior to synthesis and the other to synthesise the observed patterns of missing data without estimating the missing values.  In both cases all data to be synthesised are treated as known and they are used to create the synthetic data which are then used for inference. The data collection agency generates multiple synthetic data sets and inferences are obtained by combining the results of models fitted to each of them. The formulae for the variance of estimates from synthetic data are different from those used for imputed data."

"Our aim in writing the synthpop package (Nowok, Raab, Snoke, and Dibben 2016) for R (R Core Team 2016) is a more modest one of providing test data for users of confidential datasets. Note that currently all values of variables chosen for synthesis are replaced but this will be relaxed in future versions of the package. These test data should resemble the actual data as closely as possible, but would never be used in any final analyses. The users carry out exploratory analyses and test models on the synthetic data, but they, or perhaps staff of the data collection agencies, would use the code developed on the synthetic data to run their final analyses on the original data. This approach recognises the limitations of synthetic data produced by these methods. "

## Synthesis diagnostics

The original Cleveland dataset contains n = 303 observations over ... 

```{r}
# data synthesis, including summary() and compare.syn()

```

\newpage

# BUILDING MODELS

Our literature review revealed that of the many approaches that have been taken, certain types of models stand out in terms of their performance.

```{r}
knitr::kable(studies, caption="Previous Approaches Documented by Shouman")
```

General performance numbers...

```{r}
knitr::kable(study_summ, caption="Average Performance by Technique")
```


## Logistic regression

The logistic regression model involves picking one or more variables in comparison to the target. All of the available variables were initially compared to the target variable in both the original data and the synthetic data. The result of this comparison in the original data was that the patient's sex, chest pain type, and the number of vessels colored by fluoroscopy were the most influential of the variables. In the synthetic data, the major influencers were the patient's age, sex, chest pain type, serum cholesterol, and the number of vessels colored by fluoroscopy.

Multiple models were tested for their validity for each type of data. For the original data, the first model tested was looking at the individual influence of patient's sex, chest pain type, and the number of vessels colored by fluoroscopy on the target variable. The second model tested examined the relationship between the patient's sex and chest pain type, and how both of these variables interacted with each other, and then the number of vessels colored by fluoroscopy when compared with the target variable. The third model looked at all numeric variables compared to the target, and the last model looked at all categorical (factorized) variables compared to the target.

```{r}
knitr::kable(data.frame("R2"=c(0.43, 0.44, 0.29, 0.57), "Adj. R2"=c(0.41, 0.41, 0.27, 0.54), "AIC"=c(235.6967, 237.9943, 283.0819, 182.3636), "BIC"=c(270.6273, 283.4041, 307.5334, 248.7317), row.names=c("Select 1", "Select 2", "Numeric", "Factors")))
```

Across the original data, the model with the best preliminary performance was the factorized variables. 

A similar process occured with the synthetic data that resulted in three models instead of four. The first model compared the patient's age, sex, chest pain type, serum cholesterol, and the number of vessels colored by fluoroscopy to the target variable. The second model looked at all numeric variables compared to the target, and the last model looked at all categorical (factorized) variables compared to the target.

```{r}
knitr::kable(data.frame("R2"=c(0.40, 0.18, 0.42), "Adj. R2"=c(0.39, 0.18, 0.42), "AIC"=c(4605.077, 6090.317, 4417.274), "BIC"=c(4682.913, 6135.722, 4540.515), row.names=c("Select", "Numeric", "Factors")))
```

The synthetic model proves once again the factorized variables are more influential than the others tested.

To further single out the best model, two kinds of predictions were made for each model: predictions against the test data for the data the model was created from, and predictions against the test data for the data the model was not created from.

```{r fig.caption="Accuracy and Statistics for Each Confusion Matrix"}
knitr::kable(lr_compared)
```

The most accurate model for the original data and the synthetic data was the factorized model. The most accurate model for data not matching what the model was built on was also the factorized model. The best of the best was definitely the factorized model for the original data.

```{r fig.cap="Confusion Matrix for Factorized Model on Original Data"}
grid.table(lm_factors_pred_orig_confusion$table)
```

## Decision tree (CHAID or C&RT?)


## Random forest

Random Forest consists of numerous decision trees that are generated based on bootstrap sampling from the In-sample data.  Subsampling reduces the variance of trees substantially and the random feature selection decorrelates them to improve the predictive accuracy and control over-fitting.
  
The bootstrap resampling of the data for training each tree increases the diversity between the trees.  Each tree is composed of a root node, branch nodes, and leaf nodes.  For each node of a tree, the optimal node splitting feature is selected from a set of features that are again randomly selected.  The final output is an ensemble of random forest trees, so that classification can be performed via majority vote.

Tuning hyperparameter values in the model significantly impact the accuracy of model performance.  The three parameters that we tune in this experiment are the number of trees, maximum depth, and the number of features to randomly select.   We use a coarse to fine search strategy for hyperparameter tuning.  Our model used normally distributed random values for parameters for a few tries. Then, we incorporate cross-validation during the training process to evaluate the results.  Upon completion, the parameters from the first pass models are used to create a refined range for parameter selection.

### Hyper parameter tuning

#### Tune using caret

The caret package in R provides an excellent facility to tune machine learning algorithm parameters. Not all machine learning algorithms are available in caret for tuning. The choice of parameters is left to the developers of the package. Only those algorithm parameters that have a large effect are available for tuning in caret. As such, only `mtry` parameter is available in caret for tuning. The reason is its effect on the final accuracy and that it must be found empirically for a dataset. The `ntree` parameter is different in that it can be as large as you like, and continues to increases the accuracy up to some point. It is less difficult or critical to tune and could be limited more by compute time available more than anything.

##### Random Search

One search strategy that we can use is to try random values within a range. This can be good if we are unsure of what the value might be and we want to overcome any biases we may have for setting the parameter (like the suggested equation above). Let us try a random search for `mtry` using caret:

We can see that the most accurate value for mtry was `r rf.random$results[[1]][1]` with an accuracy of `r rf.random$results[[2]][1]`

##### Grid Search

Grid search experiments are common in the literature of empirical machine learning, where they are used to optimize the hyper-parameters of learning algorithms.  It is common to perform multi-stage automated grid experiment, however, fine resolution for optimization would be computationally expensive.  Grid search experiments allocates many trials to the exploration of dimensions that may not matter and suffer from poor coverage in dimensions that are important.

We can see that the most accurate value for `mtry` was `r rf.gridsearch$results[[1]][1]` with accuracy of `r rf.gridsearch$results[[2]][1]`

#### Tune Using Algorithm Tools

Some algorithms provide tools for tuning the parameters of the algorithm. For example, the random forest algorithm implementation in the randomForest package provides the tuneRF() function that searches for optimal mtry values given your data. 

You can see that the most accurate value for mtry was `r bestmtry[1,][[1]]` with an OOBError of `r bestmtry[1,][[2]]`

This does not really match up with what we saw in the caret repeated cross validation experiment above, Nevertheless, it is an alternate way to tune the algorithm. 

#### Craft your own parameter search

##### Tune Manually

We want to keep using caret because it provides a direct point of comparison to our previous models and because of the repeated cross validation test harness that we like as it reduces the severity of overfitting. One approach is to create many caret models for our algorithm and pass in a different parameters directly to the algorithm manually. Let’s look at an example doing this to evaluate different values for `ntree` while holding mtry constant.

```{r}
dotplot(manual.rf)
```

You can see that the most accuracy value for ntree was perhaps 1000 with a mean accuracy of 80.38% (a lift over our very first experiment using the default mtry value).

The results perhaps suggest an optimal value for ntree between 2000 and 2500. Also note, we held mtry constant at the default value. We could repeat the experiment with a possible better mtry=2 from the experiment above, or try combinations of of ntree and mtry in case they have interaction effects.

##### Extend Caret

Another approach is to create a “new” algorithm for caret to support. This is the same random forest algorithm you are using, only modified so that it supports multiple tuning of multiple parameters. A risk with this approach is that the caret native support for the algorithm has additional or fancy code wrapping it that subtly but importantly changes it’s behavior. You many need to repeat prior experiments with your custom algorithm support. We can define our own algorithm to use in caret by defining a list that contains a number of custom named elements that the caret package looks for, such as how to fit and how to predict. See below for a definition of a custom random forest algorithm for use with caret that takes both an mtry and ntree parameters. Now, let’s make use of this custom list in our call to the caret train function, and try tuning different values for ntree and mtry.

```{r}
plot(custom.result)
```

You can see that the most accurate values for ntree and mtry were 1500 and 2 with an accuracy of 84.43%. We do perhaps see some interaction effects between the number of trees and the value of ntree. 

### Random Forest Baseline Model

we will stick to tuning two parameters, the `mtry` and the `ntree` that have the most influence on accuracy of RF model. `mtry` is the number of variables randomly sampled as candidates at each split.  `ntree` is the number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times.

Let us create a baseline for comparison, using recommended default values for parameters: `mtry` floor(sqrt(ncol(x))) and `ntree` 500. 

The baseline model used total 303 samples which includes 13 predictor variables to classify two classes in target. Resampling was done using cross-validation for 10 fold, repeating 3 times.

The estimated accuracy is `r rf.baseline$results[[2]]`

### Random Forest Final Model And Evaluation

The final model is developed by using generated synthetic data with the help of minority class data. Simply put, it takes the minority class data points and creates new data points which lie between any two nearest data points joined by a straight line. In order to do this, the algorithm calculates the distance between two data points in the feature space, multiplies the distance by a random number between 0 and 1 and places the new data point at this new distance from one of the data points used for distance calculation. Note the number of nearest neighbors considered for data generation is also a hyperparameter and can be changed based on requirement.


```{r}
dplyr::select(eval_mods, Sensitivity, Specificity, Precision, Recall, F1)
```


```{r}
# Plot cross validation verses model producers accuracy
par(mfrow=c(2,4)) 
plot(rf.org.clf.cv, type = "cv", main = "org CV producers accuracy")
plot(rf.org.clf.cv, type = "model", main = "org Model producers accuracy")
plot(rf.org.clf.cv, type = "cv", stat = "oob", main = "org CV oob error")
plot(rf.org.clf.cv, type = "model", stat = "oob", main = "org Model oob error")

plot(rf.syn.clf.cv, type = "cv", main = "syn CV producers accuracy")
plot(rf.syn.clf.cv, type = "model", main = "syn Model producers accuracy")
plot(rf.syn.clf.cv, type = "cv", stat = "oob", main = "syn CV oob error")
plot(rf.syn.clf.cv, type = "model", stat = "oob", main = "syn Model oob error")	  
```

```{r}
par(mfrow=c(1,2)) 
plot(ROCRPref.org, colorize=TRUE, print.cutoffs.at = seq(0.1, by=0.1), main = "ROC curve of Original")
plot(ROCRPref.syn, colorize=TRUE, print.cutoffs.at = seq(0.1, by=0.1), main = "ROC curve of Synthesized")
```


See apendices for the developing process of the final model.


## Support Vector Machines

Per our literature review, Assari et al found that Support Vector Machine (SVM) models provided better accuracy than Naive Bayes and Decision Tree approaches.  

SVM models are a form of supervised learning used to create discriminative large-margin classifiers, defining a decision boundary based on a hyperplane.  This hyperplane maximizes the margin, or distance, between the nearest points of different labeled classes while prioritizing correct classification.  The classifier can then be used to predictively categorize unlabeled examples.  

SVMs perform well in high dimensions and can be used to perform linear as well as non-linear classification based on the kernel function employed.  However, they are prone to overfitting and can be hard to interpret, particularly when in higher dimensions.

### Kernel Methods

Kernels are algorithms that operate in high-dimensional feature space without explicitly mapping data to the coordinates of that space.  Mathematically, this 'kernel trick' means scalar solutions to dot products can be used without computationally laborious transformations.  

Practically for SVM models, if we can map the feature space to higher dimensions in which the classes of cases become separable then a linear classifier can be used to solve non-linear problems, 

As kernel values depend on the inner products of feature vectors, it's best pracitce to scale variables to range of [0, 1] or [-1, 1]. 

Choice of kernel function has a significant impact on the outcome of an SVM model.  While this provides flexibility it also create potential points of failure, and SVM models are very sensitive to selection of kernel parameters.

Hsu et al recommend commencing with the Guassian radial basis function (RBF) kernel, which can handle nonlinear relationships, has fewer hyperparameters, and prevents fewer numerical challenges.
We evaluated both RBFs and linear kernels for the original dataset, and applied the stronger performer to the synthetic dataset.


### Tuning

A hard margin constraint prevents the classifier from allowing the margin to overlap with values.  This constraint can be relaxed, which reduces variance but adds bias.  By manipulating the penalty parameter C we can decrease (soft margin) or increase (hard margin) the weight of values within the margin on overall error.  To test the impact of different C values we can employ a grid search.


### Approach

SVM can be computationally intensive, so we first built the model and evaluated performance based on the original dataset of 303 cases before attempting with a larger synthesized dataset.  We harnessed different kernel functions and tuned parameters to optimize performance based on accuracy (number of correct predictions as percentage of total cases) and kappa statistics (comparing observed and expected accuracy based on random chance).


###



### References

https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf
https://stats.stackexchange.com/questions/225409/what-does-the-cost-c-parameter-mean-in-svm/225553
https://www.datacamp.com/community/tutorials/support-vector-machines-r
http://dataaspirant.com/2017/01/13/support-vector-machine-algorithm/




## Naive Bayes

Naive Bayes classifier assumes that the presence (or absence) of a particular feature is unrelated to the presence (or absence) of any other feature. It considers all variables to independently contribute to the probability of heart disease. In spite of their naive design and apparently oversimplified assumptions, naive Bayes classifiers often work much better in many complex real world situations. Additionally, it requires a small amount of training data to estimate the parameters. 

We removed  `age` and  `sex` from the clasifier to imrove our model. Additionally,  `chol` variable was converted into a categorical variable. The model accurasy is now 0.8746.

```{r}
#confusionMatrix_nb
```

\newpage

# MODEL REVIEW AND SELECTION

## Comparison of performance between models

[sensitivity, specificity, accuracy, others metrics?]

[Between different techniques and between original dataset and synthesize dataset for each technique]


## Comparison of performance viz. other studies


\newpage

# CONCLUSIONS


\newpage

# APPENDIX


## Supplemental tables and figures


```{r t1}
knitr::kable(vars, caption="Data Dictionary")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```


```{r}
knitr::kable(orig_data_stats , caption="Summary statistics for numerical variables in the original dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
knitr::kable(data_cat_stats, caption="Summary statistics for categorical variables in the original dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
knitr::kable(data_cat_stats_b, caption="Summary statistics for binary categorical variables in the original dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
knitr::kable(syn_data_stats , caption="Summary statistics for numerical variables in the synthesised dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
knitr::kable(syn_cat_stats , caption="Summary statistics for categorical variables in the synthesised dataset")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```



```{r}
knitr::kable(syn_cat_stats_b , caption="Summary statistics for binary categorical variables in the synthesised dataset")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```

```{r f1, fig.cap="Numeric and Categorical Data Distributions as a Function of TARGET", fig.height=6, fig.width=8}
hist.num
```



```{r f2, fig.cap = "Categorical Data Distributions as a Function of TARGET", fig.height=8, fig.width=8}
bar.cat
```



```{r f3, fig.cap = "Scaled Boxplots"}
scaled.boxplots
```

```{r f4, fig.cap = "Linear relationship between each numeric predictor and the target", fig.height=6, fig.width=8}
boxplots.target
```



```{r f5, fig.cap="Missing data in the synthesised dataset", fig.height=6, fig.width=6}
plot_missing(syn_df)
```

```{r f6, fig.height=9, fig.width=8}
corr.plot2
```


```{r fig.cap = "random forest hyperparameter tuning random search"}
plot(rf.random)
```

```{r fig.cap = "random forest hyperparameter tuning grid search"}
plot(rf.gridsearch)
```

```{r fig.cap = "random forest best mtry search"}
plot(bestmtry)
```

```{r fig.cap = "random forest manual search"}
dotplot(manual.rf)
```

```{r fig.cap = "random forest custom search"}
plot(custom.result)
```


## R statistical programming code





The appendix is available as script.R file in `projectFinal_heart` folder.

https://github.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining

```
```
