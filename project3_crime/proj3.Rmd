---
title: "CUNY SPS DATA 621 - CTG5 - HW3"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "April 10th, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 5
        fig_height: 4
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
source("./source/libs.R")
source("./source/script.R")
source("./source/captioner.R")
set.seed(123)
```


\newpage


# DATA EXPLORATION

Relocating to a new city or state can be very stressful. In addition to the stress of packing and moving, you may also be nervous about moving to an unfamiliar area. To better understand their new community, some new residents or people interested in moving to a new city choose to review crime statistics in and around their neighborhood. Crime rate may also influence where people choose to live, raise their families and run their businesses; many potential new residents steer clear of cities with higher than average crime rates.

Data was collected in order to predict whether the neighborhood will be at risk for high crime levels. For each neighborhood the response variable, `target`, represents whetever the crime rate is above the median crime rate or not.  In addition to that 13 predictor variables were collected representing each neighborhood's: proportion of large lots, non-retail business acres, whether or not it borders the Charles River, nitrogen oxides concentration, average number of rooms per dwelling, proportion of owner-occupied units, distances to five Boston employment centers, accessibility to radial highways, property tax rate, pupil-teacher ratio, proportion of african Americans, percent lower status, and median value of homes. The evaluation data contains the same 13 predictor variables and no target variable so it will be impossible to check the accuracy of our predictions from the testing data.  


VARIABLE NAME  |  DEFINITION  |         TYPE
-----------  |  -------------------------------------------  |  -----------------
target  | whether the crime rate is above the median crime rate (1) or not (0)    |     response variable
zn  | proportion of residential land zoned for large lots (over 25000 square feet)      |    predictor variable
indus  |  proportion of non-retail business acres per suburb  | predictor variable 
chas  |  a dummy var. for whether the suburb borders the Charles River (1) or not (0)  |  predictor variable
nox  |  nitrogen oxides concentration (parts per 10 million) |  predictor variable 
rm  |  average number of rooms per dwelling  |  predictor variable 
age  |  proportion of owner-occupied units built prior to 1940  |  predictor variable
dis  |  weighted mean of distances to five Boston employment centers  |  predictor variable
rad  |  index of accessibility to radial highways  |  predictor variable
tax  |  full-value property-tax rate per $10,000 |  predictor variable
ptratio  |  pupil-teacher ratio by town |  predictor variable 
black  | $1000(B_k - 0.63)^2$ where $B_k$ is the proportion of blacks by town  |  predictor variable
lstat  | lower status of the population (percent)  |  predictor variable
medv  |  median value of owner-occupied homes in $1000s  |  predictor variable



## Summary Statistics

```{r t1}
knitr::kable(sum_stat, caption="Summary statistics")
```

Looking at the Table 2, we can see that `chas` and `target` are binary variables. 49% of our target variable is coded as 0's indicating that the crime rate is NOT above the median crime rate. There are potential outliers present in `zn`, `lstat`, `medv` and `dis`. 



## Shape of Predictor Distributions

`r f.ref("f1")` shows that the distribution of most of the variables seems skewed. There are some outliers in the right tail of `tax` , `rad`, `medv`, `lstat`, `dis` and left tail of `ptratio`. 

```{r f1, fig.cap="Data Distributions"}
Hist
```


## Outliers

`r f.ref("f2")` shows that there are also a large number of outliers that need to be accounted for, most significantly in `zn` and `medv` and less significantly in `lstat`, `dis` and `rm`. Since `tax` variable has values which are very large compared to other variables in the dataset, it was scaled to fit the boxplot by dividing by 10. 

```{r f2, fig.cap="Boxplots highlighting many outliers in the data."}
outlier.boxplot
```


## Missing Values

There are no missing values in any of our observations gathered across the thirteen predictor variables as can be seen in `r f.ref("f3")`.


```{r f3, fig.height=3, fig.width=3, fig.cap="Missing values"}
na.barplot
```


## Linearity

Each variable was plotted against the target variable in order to determine at a glance which had the most potential linearity before the dataset was modified.

```{r f4, fig.height=8, fig.width=8, fig.cap="Linear relationships between each predictors and the target"}
linearity
```

As can be observed in `r f.ref("f4")`, the most influential variables are the ones previously discussed to have severe outliers and skew, and their linear relationship is negative - the higher the variable, the lower the target wins. 

\newpage


# DATA PREPARATION

## Missing Values and NA Imputation

Given that the training dataset does include missing values, there's not need to make systematic corrections or imputations.


## Dealing with outliers, leverage, and influence points

While logistic regression can be more robust to leverage points (explantory variable values, which are distant on the x-axis), outliers (response variable values, which are distant on the y-axis) can exert influence which affects the curve and accuracy of target predictions.   

- `dis`, `tax` (property tax rate per $10k), and `medv` (median value of owner-occupied homes) see a few outliers and leverage points in both target classes 
- `indus` (the non-retail business acreage proportion) and `lstat` (percent lower status population) both have outliers in the below-mean (0) class
- `ptratio` (pupil-teacher ratio) fit is very impacted by density of low values in the above-mean class, making the linear relationship appear parabolic
- `rad` (highway access index) is influenced by a high-value concentration of locations distant from radial highways that fall in the above-mean class
- `rm` (average rooms per dwelling) sees a wider distribution of house size for tha above-mean class then the below-mean; while `zn` (large-lot zoned land proportion) sees the opposite, with a concentration around a few non-residential land proportions for the above-mean class and a wide dispersion for the below-mean class

```{r, f5, fig.cap="Linear relationships between each predictors and the target"}
linearity_log
```

We examined the linear relationships after a log transformation, which smoothed several relationships but still demonstrated visible influence for several variables: `lstat`, `medv`, `ptratio`, `rad`, `rm`, `tax`, and `zn`.  We discuss further in the feature engineering section below.

[JEREMY: TEAM, SO WE WANT TO DO FURTHER INVESTIGATION OF OUTLIERS, LOOKING AT R2, STANDARD ERRORS, P-VALS, AND LEVERAGE VALUES FROM THE HAT MATRIX FOR PAIRS OF MODELS, ONE THAT INCLUDES OUTLIERS AND ANOTHER THAT DOESN'T; OR IS THIS ENOUGH?]

## Correlation

```{r, f6, fig.cap="Correlation between predictors"}
correl2
```

An examination of correlation between the explanatory variables reveals the following:

- `indus` (non-retail business acre proportion) is positively correlated with `nox` (pollution concentration, $r = .76$) and `tax` (property tax rate per \$10k, $r = .73$) and is negatively correlated with `dis` (weighted mean distance to employment centers, $r = -.7$)
- `chas` (bordering Charles river) correlated with `nox` ($r = .97$) and `rm` (average rooms per dwelling, $r = .91$) and `age` (proportion of pre-1940 homes, $r = .79$); and is negatively correlated with `dis` ($r = -.97$)
- `medv` (median value of owner-occupied homes) is correlated with `rm` ($r = .71$); and is negatively correlated with `lstat` (percent lower status population, $r = -.74$)
- `age` is correlated with `nox` ($r = .74$); and is negatively correlated with `dis` ($r = -.75$)
- `rad` (highway access index) correlated with `tax` ($r = .91$)

[JEREMY: TEAM, LET'S DISCUSS HOW WE'D LIKE TO APPLY THESE CORRELATION FINDINGS TO MODEL EVALUATION AND VARIABLE SELECTION]


## Feature Engineering

In MARR, Sheather quotes Cook and Weisberg, suggesting that the best way to determine need for log transformation of skewed predictors is to include both the original and transformed varaibles in the logistic regression model in order assess their relative contributions directly and prune accordingly

Reexamining the histograms of the predictor distributions above reveals that:

- `age` is left-skewed
- `dis` is right-skewed, and `zn` is extremely so
- `nox` is right-skewed and platykurtic (thin-tailed)
- `rad` and `tax` seem to have normal distributions, with large numbers of outliers at particular levels
- `indus` and `ptratio` reveal pecular skew, with incidences at particular high level, perhaps due to regulation or infrastructure requirements

We include log transforms of `age`, `dis`, `nox`, `rad`, `tax`, `indus`, and `ptratio` in the dataset for evaluation in models.  

[JEREMY: TEAM, DO WE WANT TO ADDRESS THIS BY CALLING log() ON VARIABLES WHEN BUILDING glm(), OR SHOULD WE TRANSFORM IN SOURCE DATASET?  I'VE ASSUMED THE FORMER.]

# BUILD MODELS

Due to small number of observations for training, the K-fold cross validation is used to train with k=10. We will split the data and hold 20% for validation for modelling. When we the final model is selected, the model can be applied to the full training set.

## Model 1

$$\widehat{y} = -1.95\times zn -0.44\times indus + 0.12\times chas + 5.65\times nox -0.06\times rm + 0.71\times age + 1.47 \times dis + 5.54 \times rad -0.85 \times tax + 0.66 \times ptratio + 0.26\times lstat + 1.20 \times medv$$
The First model is the binary logistic model including all the explanatory variables. The data is centered and scaled based on the mean and standard deviation of the variables.

```{r}
summary(model.1)
```

The residual deviance is 147.10 and the AIC is 173.1. We will consider this as the baseline for all models. 

```{r, t2, fig.cap="Correlation between predictors"}
knitr::kable(vif(model.1$finalModel))
```

The review of the VIF output suggests that some variables are highly colinear and may not be necessary to build a model.

```{r, f7, fig.cap="Model 1 ROC Curve"}
plot(roc(split.validation$target, pred.1.raw), main="ROC Curve")
```
```{r}
auc(roc(split.validation$target, pred.1.raw))
```

## Model 2

The logarithmic transformation on explanatory variables is used for Model 2 in order to normalize the distribution of the explanatory variables. Model 2 also removes variables that seemed unnecessary in Model 1. 

```{r}
summary(model.2)
```

```{r}
knitr::kable(vif(model.2$finalModel))
```

Unfortunately, both Residual deviance and AIC have worsened and the log transformation may not have been the optimal choice. 


## Model 3

Model 3 removes the variables with high VIF values from Model 2.

```{r}
summary(model.3)
```

```{r}
knitr::kable(vif(model.3$finalModel))
```

[Since model 2, 3 is not developing, I thought I should try the step approach...]

Before we proceed to the next model, Consider different perspective on the built models. Use step approach to see if it is possible to build a model iteratively (forward/backward) and check if there is a room to improve.


```{r}
step(multinom(target ~ ., train), direction = "backward")
step(multinom(target ~ ., train), direction = "forward")
```

As above, judging from both Residual Deviance and AIC, the forward/backward stepwise approach did not improve the model.

## Model 4

## Evaluate Models

```{r}
knitr::kable(eval)
```

Three models were explored in order to determine the best way to determine whether or not a neighborhood's crime rate was above or below the median crime rate. The most efficient model was the second model, with the first model being somewhat efficient, and the third model being least efficient.
