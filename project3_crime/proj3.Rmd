---
title: "CUNY SPS DATA 621 - CTG5 - HW3"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "April 10th, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 5
        fig_height: 4
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
source("./source/libs.R")
source("./source/script.R")
source("./source/captioner.R")
#set.seed(123)
```


\newpage


# DATA EXPLORATION

Relocating to a new city or state can be very stressful. In addition to the stress of packing and moving, you may also be nervous about moving to an unfamiliar area. To better understand their new community, some new residents or people interested in moving to a new city choose to review crime statistics in and around their neighborhood. Crime rate may also influence where people choose to live, raise their families and run their businesses; many potential new residents steer clear of cities with higher than average crime rates.

Data was collected in order to predict whether the neighborhood will be at risk for high crime levels. For each neighborhood the response variable, `target`, represents whether the crime rate is above the median crime rate or not.  In addition to that 13 predictor variables were collected representing each neighborhood's: proportion of large lots, non-retail business acres, whether or not it borders the Charles River, nitrogen oxides concentration, average number of rooms per dwelling, proportion of owner-occupied units, distances to five Boston employment centers, accessibility to radial highways, property tax rate, pupil-teacher ratio, proportion of African Americans, percent lower status, and median value of homes. The evaluation data contains the same 13 predictor variables and no target variable so it will be impossible to check the accuracy of our predictions from the testing data.  

```{r t1}
knitr::kable(variable_descriptions, caption="Data Dictionary")
```


\newpage


## Summary Statistics

```{r t2}
knitr::kable(sum_stat, caption="Summary statistics")
```

Looking at the `r t.ref("t2")`, we can see that `chas` and `target` are binary variables. 49% of our target variable is coded as 0's indicating that the crime rate is NOT above the median crime rate. There are potential outliers present in `zn`, `lstat`, `medv` and `dis`. 

## Shape of Predictor Distributions

`r f.ref("f1")` shows that the distribution of most of the variables seems skewed. There are some outliers in the right tail of `tax` , `rad`, `medv`, `lstat`, `dis` and left tail of `ptratio`. 

Even more interestingly, for many of the predictor variables the shape of the distribution is significantly different depending on the value of the `target`.  For example, `age` is highly left skewed for homes where the crime rate is above the median crime rate (`target` = 1) while the distribution for homes where the crime rate is not above the median the distribution is normal.  Other variables with similar differences are `dis`, `indus`, `lstat`, `nox`, `ptratio`, `rad`, and `tax`.

The variable `rad` has a clear separation at about a value of 5 with almost all homes with a `rad` value less than five being in the target group coded 0 and almost all homes with a `rad` value greater than five being in the target group coded 1.  Possibly indicating that a transformation into a categorical dummy variable might be desirable.  `indus` has a similar separation at a value of about 16, but not as strikingly.

```{r f1, fig.width=8, fig.cap="Data Distributions"}
Hist_new
```

## Outliers

`r f.ref("f2")` shows that there are also a large number of outliers that need to be accounted for, most significantly in `zn` and `medv` and less significantly in `lstat`, `dis` and `rm`. Since `tax` variable has values which are very large compared to other variables in the dataset, it was scaled to fit the boxplot by dividing by 10. 

```{r f2, fig.width=8, fig.cap="Boxplots highlighting many outliers in the data."}
outlier.boxplot
```

## Missing Values

There are no missing values in any of our observations gathered across the thirteen predictor variables as can be seen in `r f.ref("f3")`.


```{r f3, fig.height=3, fig.width=3, fig.cap="Missing values"}
na.barplot
```

## Linearity

Each variable was plotted against the target variable in order to determine at a glance which had the most potential linearity before the dataset was modified.

As can be observed in `r f.ref("f4")`, all of the predictor variables seem to have an impact on the target.  With most of them having a positive impact indicating that the higher the predictor variable values are more likely to correspond to a target that is coded as 1 indicating the crime rate is above the median.  The exceptions are `dis`, `medv`, `rm`, `zn`, and possibly `chas` where the distribution of predictor variable values is higher when the target is coded 0.

We can also see that many of the predictor variables have very different variances for the two values of the target.  This is especially true for `age`, `rad`, `tax`, and `zn` and less significantly for `dis` and `nox`.

```{r f4, fig.height=8, fig.width=8, fig.cap="Linear relationships between each predictor and the target"}
boxplots
```


\newpage


# DATA PREPARATION

## Missing Values and NA Imputation

Given that the training dataset does include missing values, there's no need to make systematic corrections or imputations.

## Dealing with outliers, leverage, and influence points

While logistic regression can be more robust to leverage points (explanatory variable values, which are distant on the x-axis), outliers (response variable values, which are distant on the y-axis) can exert influence which affects the curve and accuracy of target predictions.   

- `dis`, `tax` (property tax rate per $10k), and `medv` (median value of owner-occupied homes) see a few outliers and leverage points in both target classes 
- `indus` (the non-retail business acreage proportion) and `lstat` (percent lower status population) both have outliers in the below-mean (0) class
- `ptratio` (pupil-teacher ratio) fit is very impacted by density of low values in the above-mean class, making the linear relationship appear parabolic
- `rad` (highway access index) is influenced by a high-value concentration of locations distant from radial highways that fall in the above-mean class
- `rm` (average rooms per dwelling) sees a wider distribution of house size for the above-mean class then the below-mean; while `zn` (large-lot zoned land proportion) sees the opposite, with a concentration around a few non-residential land proportions for the above-mean class and a wide dispersion for the below-mean class

The figures below examine the linear relationships after a log transformation, which smoothes several relationships but still demonstrates visible influence for several other variables: `lstat`, `medv`, `ptratio`, `rad`, `rm`, `tax`, and `zn`.  We discuss further in the feature engineering section below.

```{r f6, fig.width=8, fig.cap="Natural log transformed predictor distributions"}
Hist_log_new
```

```{r f5, fig.height=8, fig.width=8, fig.cap="Relationships between natural log transformed predictors and the target"}
linearity_log_new
```


\newpage


## Correlation

An examination of correlation between the explanatory variables reveals the following:

- `indus` (non-retail business acre proportion) is positively correlated with `nox` (pollution concentration, $r = .76$) and `tax` (property tax rate per \$10k, $r = .73$) and is negatively correlated with `dis` (weighted mean distance to employment centers, $r = -.7$)
- `chas` (bordering Charles river) correlated with `nox` ($r = .97$) and `rm` (average rooms per dwelling, $r = .91$) and `age` (proportion of pre-1940 homes, $r = .79$); and is negatively correlated with `dis` ($r = -.97$)
- `medv` (median value of owner-occupied homes) is correlated with `rm` ($r = .71$); and is negatively correlated with `lstat` (percent lower status population, $r = -.74$)
- `age` is correlated with `nox` ($r = .74$); and is negatively correlated with `dis` ($r = -.75$)
- `rad` (highway access index) correlated with `tax` ($r = .91$)

```{r f7}
#Not sure why this doesn't work if you put it in the script file...
train %>% 
  select(-target) %>% 
  cor() %>% 
  round(2) %>% 
  corrplot(method = "circle")
```

```{r t3}
kable(correl2, caption="Correlation between predictors")
```


## Feature Engineering

In 'A Modern Approach to Regression with R' (page 284), Sheather quotes Cook and Weisberg, suggesting that the best way to determine need for log transformation of skewed predictors is to include both the original and transformed variables in the logistic regression model in order assess their relative contributions directly and prune accordingly

Reexamining the histograms of the predictor distributions above reveals that:

- `age` is left-skewed
- `dis` is right-skewed, and `zn` is extremely so
- `nox` is right-skewed and platykurtic (thin-tailed)
- `rad` and `tax` seem to have normal distributions, with large numbers of outliers at particular levels
- `indus` and `ptratio` reveal peculiar skew, with incidences at particular high level, perhaps due to regulation or infrastructure requirements

We include log transforms of `age`, `dis`, `nox`, `rad`, `tax`, `indus`, and `ptratio` in the dataset for evaluation in models.  


\newpage


# BUILD MODELS

## Model 1

$$
\begin{aligned}
\widehat{y} = & -1.95\times zn -0.44\times indus + 0.12\times chas + 5.65\times nox -0.06\times rm + 0.71\times age \\
& + 1.47 \times dis + 5.54 \times rad -0.85 \times tax + 0.66 \times ptratio + 0.26\times lstat + 1.20 \times medv
\end{aligned}
$$

The First model is the binary logistic model including all the explanatory variables.  The data is centered and scaled based on the mean and standard deviation of the variables.

```{r t4}
summary(model.1)
```

The residual deviance is 192.05 and the AIC is 218.05. We will consider this as the baseline for all models. 

### Variance Inflation Factors

```{r t5}
knitr::kable(vif(model.1$finalModel), caption="Variance Inflation Factors for Model 1")
```

The review of the VIF output suggests that some variables are highly collinear and may not be necessary to build a model.

```{r f8, fig.cap="Model 1 ROC Curve"}
plot(roc(train$target, pred.1.raw), main="ROC Curve")
```

```{r f9}
auc(roc(train$target, pred.1.raw))
```


## Model 2

The second model is a binary logistic model including all the explanatory variables plus log transformations of our skewed variables `age`, `dis`, `nox`, `rad`, `tax`, `indus`, and `ptratio` as recommended by Sheather in 'A Modern Approach to Regression with R'.

```{r t6}
summary(model.2)
```

### Marginal Model Plots

```{r eval=FALSE, fig.height=8, fig.width=8, include=FALSE}
mmps(model.2, layout=c(5,4), key=NULL) 
```


### Variance Inflation Factors

```{r t7}
knitr::kable(vif(model.2), caption="Variance Inflation Factors for Model 2")
```


## Model 3

Model 3 removes the variables with high VIF values from Model 2.

```{r t8}
summary(model.3)
```

### Variance Inflation Factors

```{r t9}
knitr::kable(vif(model.3), caption="Variance Inflation Factors for Model 3")
```

Before we proceed to the next model, Consider different perspective on the built models. Use step approach to see if it is possible to build a model iteratively (forward/backward) and check if there is a room to improve.

### Backward Elimination

```{r}
backward
```

### Forward Selection

```{r}
forward
```

As you can see above, judging from both Residual Deviance and AIC, the forward/backward stepwise approach did not improve the model.

## Model 4

The first step was to create a logit model, which includes all variables in the training data set. In the second step we are performing backward elimination. The remaining variables are: distance to employment centers (negative effect), accessibility to radial highway (positive effect), and proportion of owner-occupied units built prior to 1940 (positive effect). 

```{r t10}
summary(model.4)
```

### Variance Inflation Factors

```{r t11}
knitr::kable(vif(model.4), caption="Variance Inflation Factors for Model 4")
```


## Model 5

```{r}
mod5_summary
```

```{r eval=FALSE, include=FALSE}
residual.plots(model.5, layout = c(6, 5))
```

```{r eval=FALSE, include=FALSE}
mmps(model.5, span = 3/4, layout = c(6, 6))
```

### Variance Inflation Factors

ALL of the variance inflation factors FAR exceed 5, the cut-off often used, and so the associated regression coefficients are poorly estimated due to multicollinearity.

```{r t12}
knitr::kable(vif(model.5), caption="Variance Inflation Factors for Model 5")
```


\newpage


# SELECT MODELS

```{r t13, eval=FALSE, include=FALSE}
knitr::kable(eval_mods)
```

The models were explored in order to determine the best way to determine whether or not a neighborhood's crime rate was above or below the median crime rate. The most efficient model was the second model, with the first model being somewhat efficient, and the third model being least efficient.
