---
title: "CUNY SPS DATA 621 - CTG5 - HW4"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "April 24th, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 8
        fig_height: 6
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
options(scipen=999, digits = 2)
source("./source/libs.R")
source("./source/script.R")
source("./source/captioner.R")
#set.seed(123)
```

\newpage

# DATA EXPLORATION

In this assignment we explore, analyze and model a dataset containing 8,161 observations with 25 variables each representing a customer at an auto insurance company.  Two of the 25 features are target variables and 23 are predictors.  One of the target variables, `TARGET_FLAG`, is a binary categorical variable where a value of 1 indicates that the customer has made a claim related to a car crash and a value of 0 indicates they have not.  The other target variable, `TARGET_AMT`, is a continuous numerical variable representing the payout amount of a claim, if any.  Of the remaining 23 predictor variables, 13 are categorical and 10 are numerical.  

Using this data, we will compose and evaluate several types of models with the following objectives:
- Logistic classification models that aim to predict the probability that a person crashes their car
- Multiple linear regression models that aim to predict the amount of money it will cost if the person does crash their car

The intended use case for these models is actuarial in nature: specifically, to calculate insurance rates commensurate with policyholders' (or policy applicants') potential risk levels, based on attributes such as income, age, distance to work,  tenure as customers, etc.

```{r t1}
knitr::kable(vars, caption="Data Dictionary")
```

Inspection of the target variables reveals that where `TARGET_FLAG` has values of 0 (i.e. no claim) `TARGET_AMT` also has values of 0 (i.e. no payout), which is logically consistent. 

## Summary Statistics

We summarize continuous and categorical variables separately.

```{r t2.1}
knitr::kable(summary.stat.num, caption="Summary statistics")
```

`EDUCATION`, `JOB`, `CAR_TYPE`, `KIDSDRIV`, `HOMEKIDS`, AND `CLM_FREQ` each comprise multiple categories.  On the other hand, `PARENT1`, `SEX`, `MSTATUS`, `CAR_USE`, RED_CAR`, `REVOKED`, URBANICITY` are all binaries.

```{r t2.2}
knitr::kable(summary.cat1, caption="Summary statistics for Categorical Variables")
knitr::kable(summary.cat2, caption="Summary statistics for Binary Categorical Variables")
```

Examining the dispersion of claims between variables, it looks like likelihoods are higher for drivers who are male, urban, blue collar, unmarried, or parents; as well as for those with commercial vehicles or a revoked license. 

```{r fig.cap="Numeric Data Distributions as a Function of TARGET_FLAG"}
hist.num
```


```{r fig.cap = "Categorical Data Distributions as a Function of TARGET_FLAG", fig.height=8}
bar.cat
```

The scale of continuous variables' distributions are considerably different and difficult to visualize together.

```{r fig.cap = "Outliers Boxplot", eval=FALSE}
outlier.boxplots
```

Scaling the distribution of based on the standard deviation reveals that outliers are very abundant for certain continuous variables, particularly `OLDCLAIM`, `INCOME`, `TRAV_TIME`, `BLUEBOOK`, and to a lesser extent `HOME_VAL` and `TIF`.

```{r fig.cap = "Scaled Boxplots"}
scaled.boxplots
```

[JO: WEREN'T WE GO TO TOSS THE NEXT CHART?]

```{r fig.cap = "Linear relationship between each numeric predictor and the target"}
boxplots.target
```

## Linearity

[JO: DON'T THINK THE LOG TRANSFORM DISAMBIGUATES LINEAR RELATIONSHIPS - IF TEAM ALIGNED, WE CAN TRY TO ADD REGRESSION EQUATIONS TO THE FACETS:  https://community.rstudio.com/t/annotate-ggplot2-with-regression-equation-and-r-squared/6112/6]

```{r fig.cap="Scatter plot between numeric predictors and the TARGET_AMT", fig.height=9}
linearity
```

[JO: DON'T THINK THE LOG TRANSFORM HAS HELPED - THINK WE SHOULD ADD]

```{r fig.cap="Scatter plot between log transformed numeric predictors and the log transformed TARGET_AMT", fig.height=9}
linearity.log
```

## Missing Data

```{r fig.cap="Missing data"}
plot_missing(train.raw)
```

A number of variables are missing observations: `AGE`, `INCOME`, `YOJ`, `HOME_VAL`, `CAR_AGE`. For `AGE`, the number is not consequential, but the others range between 5% and 6% of total.

[JO: WHERE DID WE NET OUT ON CHECKING WHETHER THE SAME OBSERVATIONS ARE MISSING THE ABOVE VARIABLES AND WHETHER THEY HAVE ANYTHING IN COMMON I.E. THEY'RE ALL MARRIED, HIGH EARNERS, HOMEOWNERS?]

We will impute missing values when preparing the data for modelling.

\newpage


# DATA PREPARATION

## Variable Descriptions

#### KIDSDRIV
`KIDSDRIV` is a categorical predictor with values ranging from 0 to 4. It shows heavy skew, with most cars having no kid drivers (value of 0).  Judging from the distribution, it appears that having kid driver results in higher probability of making a claim.

#### AGE
`AGE` presents driver's age and shows a normal distribution centered around 45 years.  Looking at the boxplot of age, there does not appear to be a difference in the distribution between whether a claim is made or not. Accordingly, this `AGE` may not be helpful in determining the probability of making a claim.

#### HOMEKIDS
`HOMEKIDS` is a predictor describing number of children at home ranging from 0 to 5. 

#### YOJ
`YOJ` is a predictor describing years on job. People who stay at a job for a longer time are believed to be safer drivers. Apart from those who are unemployed (values of 0), `YOJ` seems to show a normal distribution. 

#### INCOME
`INCOME` is a heavily skewed predictor variable, suggesting that outliers should be treated for modelling.

#### HOME_VAL
`HOME_VAL` is a home value predictor variable. In theory, home owners tend to drive more responsibly. The difference between owners and renters (values of 0) is visible in the graph.

#### TRAVTIME
`TRAVTIME` is a predictor variable describing the distance to work. Long drives to work would suggest greater risk of an accident and claim. However, the graph shows a fairly normal distribution, such that this variable may not be helpful in determining the probability of making a claim.

#### BLUEBOOK
`BLUEBOOK` is a predictor variable describing the value of the car. The`boxplot demonstrates that the lower value of the car, the higher chances of making a claim. It is conceivable that higher-priced cars are driven more carefully.

#### TIF
`TIF` describes how long the customer has been with the insurance company.  Plots reveal that the longer the tenure of a policyholder, the lower the likelihood of a claim - i.e. safe drivers tend to remain so.

#### OLDCLAIM
`OLDCLAIM` is a predictor describing the value of claims made in the past 5 years. It is very heavily skewed as most policyholders do not make claims.

#### CLM_FREQ
`CLM_FREQ` is a predictor that describing the frequency of claims in the past 5 years. It suggests that those who have made a claim in the past 5 years are more likely to make another claim.

#### MVR_PTS
`MVR_PTS` is a predictor that describes motor vehicle record points. The rationale is that more traffic tickets suggests less safe driving and a higher likelihood of claims. It appears to be a highly significant variable as seen in boxplots.

#### CAR_AGE
`CAR_AGE` describes the age of the policyholder's vehicle. One value is -3, which must be an error - this is corrected to 0. 

#### PARENT1
`PARENT1` indicates whether a policyholder is a single parent. This variable has been factorized and relabeled as `NumParents` to describe the number of parents. 

#### SEX
`SEX` describes the gender of the driver. This variable has been factorized and relabeled as `MALE, for which males receive a value of 1 and females a value of as 0. It does not appear to be significant variable in the box plot.

#### MSTATUS
`MSTATUS` describes the marital status of the policyholder. The rationale is that married people drive more safely. This variable has been factorized and relabeled as `Single`, for which married policyholders receive a value of 0 and unmarried a value o 1.

#### EDUCATION
`EDUCATION` describes the education level of the driver. This variable is factorized. It may be correlated with INCOME.
	
#### JOB
`JOB` describes the type of job the driver has. This variable is factorized. It may be correlated with INCOME. In theory policyholders with white collar jobs tend to drive more safely.

#### CAR_TYPE
`CAR_TYPE` describes type of car. This variable is factorized. 

#### CAR_USE
`CAR_USE` describes how the vehicle is used. Commercial vehicles are driven more and may have an elevated probability of accidents and claims. This variable is factorized and relabeled as `Commercial`, for which a value of 0 means private use and a value of 1 means commercial use.

#### RED_CAR
`RED_CAR` indicates whether the color of the vehicle is red. Red vehicles, especially sports cars, are associated with riskier driving and likelihood of claims. This variable is factorized.

#### REVOKED
`REVOKED` describes whether a policyholders license has been revoked in the past 7 years. License revocation is associated with riskier driving. This variable is factorized. The boxplot reveals that policyholders who previously lost their license are more likely to file claims.

#### URBANICITY
`URBANICITY` describes whether driver lives in Urban area or Rural area. This variable has been factorized and relabeled as `URBAN`, for which a value of 0 means rural and a value of 1 means urban.


## Missing values

[JO: THINK WE SHOULD DESCRIBE PURPOSE OF/ NEED FOR VALUE IMPUTATION.  MICE IMPUTATION ASSUMES 'MISSING AT RANDOM' (MAR), SO THINK WE'LL NEED TO ESTABLISH THAT THIS IS THE CASE.]
[JO: WHAT'S THE DIFFERENCE BETWEEN THE M AND MAXIT VALUES (1 FOR AGE, 2 FOR OTHERS?)]

To deal with missing data values for the variables `INCOME`, `YOJ`, `HOME_VAL`, and `CAR_AGE - and to a lesser extent `AGE` - we leveraged the MICE (Multivariate Imputation By Chained Equations) package. The package creates multiple imputations (replacement values) for multivariate missing data using a a method based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. The method can impute mixes of continuous, binary, unordered categorical and ordered categorical, and continuous two-level data; and it can maintain consistency between imputations by means of passive imputation. We inspected the quality of imputed values using multiple diagnostic plots.

```{r fig.cap = "Difference between original and imputed data"}
density.plot
```

The blue and red lines represent the distribution originally known and imputed values respectively. With the exception of `AGE`, the distribution of the imputed values accords with the distribution of where values are not missing. For those four variables we will use the imputed values.  For `AGE` (only .07 or XX number of cases) we will impute `AGE` separately using the median imputation.

```{r, fig.height=9, eval=FALSE}
#corr.table
```

```{r, fig.height=9}
corr.plot2
```

Unsurprisingly, higher levels of `INCOME` are found with higher values of `YOJ`; this also means more income is disposable, which shows correlation with HOME_VAL and BLUEBOOK.
Additionally, `MVR_PTS` shows a relationship with `OLDCLAIMS`.

```{r, eval=FALSE}
corr.train <- train.num.a %>%
  dplyr::select(-TARGET_FLAG) %>%
  dplyr::select(-TARGET_AMT) %>%
  cor() %>%
  round(2) %>%
  corrplot(method = "circle")
```

```{r, eval=FALSE}
knitr::kable(corr.train, caption="Correlation table")
```

```{r fig.width = 6, fig.height=6}
corr.plot
```

```{r, fig.height=9, eval=FALSE}
pairs.train <- train.num.a %>%
  dplyr::select(-TARGET_AMT)
group <- NA
group[pairs.train$TARGET_FLAG == 0] <- 1
group[pairs.train$TARGET_FLAG == 1] <- 2
pairs(pairs.train, col=c("#58BFFF", "#3300FF")[group], pch = c(3, 1)[group])
```


\newpage


# BUILD MODELS

## Classification Models: Model 1,2,3,4

The first four models take categorical variables as inputs and interpret their contributions to predicting the likelihood of a claim [JO: CONFIRM JUST 'for new customers'?]. We use `drop` and `MASS:stepAIC` functions to judge which variables to remove, evaluating AIC statistics as we go.

### MODEL 1

__ TARGET_FLAG ~ NumParents+ Male+ EDUCATION+ JOB+ CAR_TYPE+ RED_CAR+ REVOKED+ Urban+ Single+ Commercial __

For an easily interpretable model aimed at predict TARGET_FLAG, we restrict inputs for Model 1 to categorical variables alone. The AIC metric suggests that the `RED_CAR` variable can be removed.

<!-- ```{r} -->
<!-- summary(model.1) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- drop1(mod.1) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- MASS::stepAIC(mod.1, trace=0) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- knitr::kable(vif(model.1$finalModel)) -->
<!-- ``` -->

### MODEL 2

__TARGET_FLAG ~ KIDSDRIV+ AGE+ HOMEKIDS + YOJ+INCOME+HOME_VAL+ TRAVTIME+ BLUEBOOK+ TIF+OLDCLAIM+ CLM_FREQ+ MVR_PTS+ CAR_AGE + PARENT1+ SEX+ EDUCATION+ JOB+ CAR_TYPE+ REVOKED+ URBANICITY+ MSTATUS+ CAR_USE

As suggested by the Model 1 finding, Model 2 excludes the `RED_CAR` variable. AIC metrics suggest removing `AGE`, `CAR_AGE` and `SEX`.

<!-- # ```{r} -->
<!-- # summary(model.2) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # drop1(mod.2) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # MASS::stepAIC(mod.2, trace=0) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # knitr::kable(vif(model.2$finalModel)) -->
<!-- # ``` -->


### MODEL 3

__TARGET_FLAG ~ KIDSDRIV+ HOMEKIDS + YOJ+INCOME+HOME_VAL+ TRAVTIME+ BLUEBOOK+
              TIF+OLDCLAIM+ CLM_FREQ+ MVR_PTS+ PARENT1+ EDUCATION+ JOB+ CAR_TYPE+
              REVOKED+ URBANICITY+ MSTATUS+ CAR_USE__

Building on model 2, model 3 excludes `AGE`, `CAR_AGE` and `SEX`.

[RJ: Anybody wants to create classification evaluation table in the select model section for those? -- might want to roll back for the data cleaning I had done previously as we need the factorized values to create that matrices.]

<!-- ```{r} -->
<!-- summary(model.3) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- drop1(mod.3) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- MASS::stepAIC(mod.3, trace=0) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- knitr::kable(vif(model.3$finalModel)) -->
<!-- ``` -->


### Model 4 - Binary logistic model

Model 4 incorporates all explanatory variables plus log transformations of skewed variables: `INCOME`, `TRAVTIME`, `BLUEBOOK`, `OLDCLAIM`, and `AGE` in a binary logistic model refined through backward elimination.

```{r fig.cap="Model 4"}
mod4_summary
```

## Regression Model: Model 5,6

The next two models are multiple linear regression models aimed at predicting the value of claims based on different approaches, including constraining the cases based on `TARGET_FLAG` (i.e. based on whether or not a claim was filed) and different approaches to selecting explanatory variables.

### Model 5 - Multiple linear regression model

Model 5 is a multiple linear regression model built only on cases with claims where `TARGET_AMT` is greater than 0.  The model is refined using stepwise elimination.  [JO: WE SHOULD ADD INTERPRETATION OF T- AND P-VALS]..  

```{r fig.cap="Model 5"}
mod5_summary
```


### Model 6 - Multiple linear regression model

Model 6 includes is a multiple linear regression model built on all cases - in other words it relaxes the constraint that a claim was filed, and so includes `TARGET_AMT` values of 0. Any predicted value less than $100 will be considered 0. [JO: WHY IS THIS?] Forward elimination is used to refine variable selection.

```{r fig.cap="Model 6"}
mod6_summary
```


# SELECT MODELS



# Appendix

The appendix is available as script.R file in `project4_insurance` folder.

https://github.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining
