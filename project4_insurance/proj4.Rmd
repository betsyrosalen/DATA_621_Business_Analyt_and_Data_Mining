---
title: "CUNY SPS DATA 621 - CTG5 - HW4"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "April 24th, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 6
        fig_height: 4
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
options(scipen=999, digits = 2)
source("./source/libs.R")
source("./source/script.R")
source("./source/captioner.R")
#set.seed(123)
```

\newpage

# DATA EXPLORATION

In the pursuit of determining relationships between car crashes, their costs, and factors that may play a role into each, a dataset containing 8,161 observations with 25 variables was explored, analyzed, and modeled. This data came from an auto insurance company with each observation representing one of their customers. Of the 25 variables, two were target variables (car crashes and car costs), and the other 23 were predictors. `TARGET_FLAG` is a binary variable where a value of 1 indicates that the customer has made a claim related to a car crash and a value of 0 indicates they have not. The other target variable, `TARGET_AMT`, is a continuous numerical variable whose value is the payout amount of a claim, if any. The remaining variables are split in their categorization; 13 are categorical and 10 are numerical.

This data was utilized to compose and evaluate several types of models with the following features:

- Logistic classification models that aim to predict the probability that a person crashes their car; and,
- Multiple linear regression models that aim to predict the amount of money it will cost if the person does crash their car.

The intended use case for these models is actuarial in nature: specifically, to calculate insurance rates commensurate with policyholders' (or policy applicants') potential risk levels based on attributes such as income, age, distance to work, tenure as customers, so on and so forth.

```{r t1}
knitr::kable(vars, caption="Data Dictionary")
```

Inspection of the target variables reveals that where `TARGET_FLAG` has values of 0 (i.e., no claim), `TARGET_AMT` also has values of 0 (i.e., no payout), which is logically consistent.

## Summary Statistics

Continuous and categorical variables were summarized separately for the sake of clarity.

```{r t2}
knitr::kable(summary.stat.num, caption="Summary statistics")
```

`EDUCATION`, `JOB`, `CAR_TYPE`, `KIDSDRIV`, `HOMEKIDS`, and `CLM_FREQ` each comprise multiple categories.  On the other hand, `PARENT1`, `SEX`, `MSTATUS`, `CAR_USE`, `RED_CAR`, `REVOKED`, `URBANICITY` are all binaries.

```{r t3}
knitr::kable(summary.cat1, caption="Summary statistics for Categorical Variables")
```
```{r t4}
knitr::kable(summary.cat2, caption="Summary statistics for Binary Categorical Variables")
```

## Variable Descriptions

#### KIDSDRIV
`KIDSDRIV` is a categorical predictor with values ranging from 0 to 4. It shows heavy skew, with most cars having no kid drivers (value of 0).  Judging from the distribution, it appears that having a kid driver results in higher probability of making a claim.

#### AGE
`AGE` presents driver's age and shows a normal distribution centered around 45 years.  Looking at the boxplot of age below, there does not appear to be a difference in the distribution between whether a claim is made or not. Accordingly, this `AGE` may not be helpful in determining the probability of making a claim.

#### HOMEKIDS
`HOMEKIDS` is a predictor describing number of children at home ranging from 0 to 5. 

#### YOJ
`YOJ` is a predictor describing years on job. People who stay at a job for a longer time are believed to be safer drivers. Apart from those who are unemployed (values of 0), `YOJ` seems to show a normal distribution. 

#### INCOME
`INCOME` is a heavily skewed predictor variable, suggesting that outliers should be treated for modelling.

#### HOME_VAL
`HOME_VAL` is a home value predictor variable. In theory, home owners tend to drive more responsibly. The difference between owners and renters (values of 0) is visible in the summary statistics graph.

#### TRAVTIME
`TRAVTIME` is a predictor variable describing the distance to work. Long drives to work would suggest greater risk of an accident and claim. However, its graph shows a fairly normal distribution, such that this variable may not be helpful in determining the probability of making a claim.

#### BLUEBOOK
`BLUEBOOK` is a predictor variable describing the value of the car. The boxplot demonstrates that the lower value of the car, the higher chances of making a claim. It is conceivable that higher-priced cars are driven more carefully.

#### TIF
`TIF` describes how long the customer has been with the insurance company.  Plots reveal that the longer the tenure of a policyholder, the lower the likelihood of a claim - i.e. safe drivers tend to remain so.

#### OLDCLAIM
`OLDCLAIM` is a predictor describing the value of claims made in the past 5 years. It is very heavily skewed as most policyholders do not make claims.

#### CLM_FREQ
`CLM_FREQ` is a predictor that describing the frequency of claims in the past 5 years. It suggests that those who have made a claim in the past 5 years are more likely to make another claim.

#### MVR_PTS
`MVR_PTS` is a predictor that describes motor vehicle record points. The rationale is that more traffic tickets suggests less safe driving and a higher likelihood of claims. It appears to be a highly significant variable as seen in boxplots.

#### CAR_AGE
`CAR_AGE` describes the age of the policyholder's vehicle. One value is -3, which must be an error - this is corrected to 0. 

#### PARENT1
`PARENT1` indicates whether a policyholder is a single parent. This variable has been factorized and relabeled as `NumParents` to describe the number of parents. 

#### SEX
`SEX` describes the gender of the driver. This variable has been factorized and relabeled as `MALE`, for which males receive a value of 1 and females a value of as 0. It does not appear to be significant variable in the boxplots below.

#### MSTATUS
`MSTATUS` describes the marital status of the policyholder. The rationale is that married people drive more safely. This variable has been factorized and relabeled as `Single`, for which married policyholders receive a value of 0 and unmarried a value o 1.

#### EDUCATION
`EDUCATION` describes the education level of the driver. This variable is factorized. It may be correlated with INCOME.
	
#### JOB
`JOB` describes the type of job the driver has. This variable is factorized. It may be correlated with INCOME. In theory policyholders with white collar jobs tend to drive more safely.

#### CAR_TYPE
`CAR_TYPE` describes type of car. This variable is factorized. 

#### CAR_USE
`CAR_USE` describes how the vehicle is used. Commercial vehicles are driven more and may have an elevated probability of accidents and claims. This variable is factorized and relabeled as `Commercial`, for which a value of 0 means private use and a value of 1 means commercial use.

#### RED_CAR
`RED_CAR` indicates whether the color of the vehicle is red. Red vehicles, especially sports cars, are associated with riskier driving and likelihood of claims. This variable is factorized.

#### REVOKED
`REVOKED` describes whether a policyholders license has been revoked in the past 7 years. License revocation is associated with riskier driving. This variable is factorized. The boxplot reveals that policyholders who previously lost their license are more likely to file claims.

#### URBANICITY
`URBANICITY` describes whether driver lives in an urban area or a rural area. This variable has been factorized and relabeled as `URBAN`, for which a value of 0 means rural and a value of 1 means urban.

### Summary Statistics Graphs

[GAB: This sentence needs rephrasing and a supporting chart... if the supporting chart is the next chart, then this needs to be/should be moved]
Examining the dispersion of claims between variables, it looks like likelihoods are higher for drivers who are male, urban, blue collar, unmarried, or parents; as well as for those with commercial vehicles or a revoked license. 

```{r f1, fig.cap="Numeric Data Distributions as a Function of TARGET_FLAG", fig.height=6, fig.width=8}
hist.num
```


```{r f2, fig.cap = "Categorical Data Distributions as a Function of TARGET_FLAG", fig.height=8, fig.width=8}
bar.cat
```

The scale of the continuous variables' distributions are considerably different and difficult to visualize together.

Scaling the distribution based on the standard deviation reveals that outliers are very abundant for the continuous variables `OLDCLAIM`, `INCOME`, `TRAV_TIME`, `BLUEBOOK`, and to a lesser extent `HOME_VAL` and `TIF`. The variables that appear to have the most outliers are `OLDCLAIM`, `BLUEBOOK`, `TRAVTIME`, and `INCOME`. All of the variables show varying levels of skew save `YOJ` and `AGE` which appear the most normally distributed.

```{r f3, fig.cap = "Scaled Boxplots"}
scaled.boxplots
```

```{r f4, fig.cap = "Linear relationship between each numeric predictor and the target", fig.height=6, fig.width=8}
boxplots.target
```

## Linearity

```{r f5, fig.cap="Scatter plot between numeric predictors and the TARGET_AMT, filtered for rows where TARGET_AMT is greater than 0", fig.height=7}
linearity
```

The plotted numeric predictors with their raw values fail to show any clear linear relationships with the `TARGET_AMT` except for the faintest of linearity in the `BLUEBOOK` variable.

[JO: DON'T THINK THE LOG TRANSFORM HAS HELPED - THINK WE SHOULD ADD]

[GAB: ADD WHAT? LOG DIDN'T HELP THOUGH YOU'RE RIGHT]

### Log Transformed Data 

```{r f6.1, fig.cap="Scatter plot between log transformed numeric predictors and the log transformed TARGET_AMT filtered for rows where TARGET_AMT is greater than 0", fig.height=7}
linearity.log
```

In an attempt to distinguish the linearity of the variables alongside the `TARGET_AMT`, all numeric predictors and the `TARGET_AMT` underwent a log transformation. As a result, the linearity of `BLUEBOOK` became more apparent, but there was no obvious influence on the linearity of any of the other variables.

### Box-Cox

Even though the linearity plots above don't show much improvement after a log transformation, a Box-Cox plot shows that a log transformation is recommended for the `TARGET_AMT`.  

```{r f6.2, fig.cap="Box-Cox Plot"}
# this doesn't work if it's in the script file
boxcox(TARGET_AMT~., data=test, lambda=seq(-0.2,0.2,by=0.1))
```

### Square Root Transformed Predictors and Log Transformed Target 

A plot of each numerical predictor square root transformed plotted against the log transformed `TARGET_AMT` as recommended by the Box-Cox plot still shows little improvement.  

```{r f6.3, fig.cap="Scatter plot between square root transformed numeric predictors and the square root transformed TARGET_AMT filtered for rows where TARGET_AMT is greater than 0", fig.height=7}
linearity.root
```

## Missing Data

```{r f7, fig.cap="Missing data", fig.height=6, fig.width=6}
plot_missing(train.raw)
```

A number of variables are missing observations: `AGE`, `INCOME`, `YOJ`, `HOME_VAL`, `CAR_AGE`. For `AGE`, the number is inconsequential, but the others range between 5% and 6% of total.  Approximately 21% of the cases are missing one of these variables, and an additional 2% are missing more than one.  For this reason, we don't suspect latent factors can account for the absences, and assume that these values are missing at random and can be imputed when preparing the data for modeling. 

\newpage


# DATA PREPARATION

## Missing Values

[JO: WHAT'S THE DIFFERENCE BETWEEN THE M AND MAXIT VALUES (1 FOR AGE, 2 FOR OTHERS?)]
[GAB: CAN I KNOW THE ANSWER TO THIS TOO?]

To deal with missing data values for the variables `INCOME`, `YOJ`, `HOME_VAL`, and `CAR_AGE` - and to a lesser extent `AGE` - the MICE (Multivariate Imputation By Chained Equations) package was leveraged. The package assumes missing values are missing at random and creates multiple imputations (replacement values) for multivariate missing data using a a method based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. The method can impute mixes of continuous, binary, unordered categorical and ordered categorical, and continuous two-level data; and it can maintain consistency between imputations by means of passive imputation. The quality of imputed values was inspected using multiple diagnostic plots.

```{r f8, fig.cap = "Difference between original and imputed data"}
density.plot
```

The blue and red lines represent the distribution originally known and imputed values respectively. With the exception of `AGE`, the distribution of the imputed values accords with the distribution of pre-existing values. The imputed values will be used for the four variables. When it comes to `AGE` (only .07 or XX number of cases), it was to be imputed separately using median imputation.

```{r f9, fig.height=9, fig.width=8}
corr.plot2
```

Unsurprisingly, higher levels of `INCOME` are found with higher values of `YOJ`; this also means more income is disposable, which shows correlation with `HOME_VAL` and `BLUEBOOK`.

Additionally, `MVR_PTS` shows a positive correlation with `OLDCLAIMS`.

```{r f10, fig.width = 6, fig.height=6}
corr.plot
```

\newpage

# BUILD MODELS

## Classification Models: Models 1, 2, 3

The first three models use the predictor variables in binary logistic models as inputs and interpret their contributions to predicting the likelihood of a claim. We use `drop` and `MASS:stepAIC` functions to judge which variables to remove, evaluating AIC statistics as we go.

### Model 1 - Base model using categorical predictors only

> TARGET_FLAG ~ PARENT1 + SEX + MSTATUS + EDUCATION + JOB + CAR_TYPE + CAR_USE + 
                REVOKED + URBANICITY + KIDSDRIV + HOMEKIDS + CLM_FREQ

For an easily interpretable model aimed at predicting TARGET_FLAG, inputs for Model 1 were restricted to categorical variables alone. The AIC metric as well as the p-value and significance code suggests that the `RED_CAR` variable could be removed, so this predictor was removed from model 1.  Model 1 serves as a base model from which to compare other models.

<!-- ```{r} -->
<!-- summary(model.1) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- drop1(mod.1) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- MASS::stepAIC(mod.1, trace=0) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- knitr::kable(vif(model.1$finalModel)) -->
<!-- ``` -->

```{r t5}
mod1_summary
```

```{r fig.cap="Model 1 ROC Curve", fig.height=3, fig.width=3.5}
plot(roc(train$TARGET_FLAG, pred.1.raw), main="ROC Curve")
```

```{r}
kable(auc(roc(train$TARGET_FLAG, pred.1.raw)), caption="Area Under the Curve")
```

### Marginal Model Plots

```{r fig.height=7, fig.width=8, fig.cap="Model 1 Marginal Model Plots"}
mmps(mod.1)
```


\newpage

### Model 2 - Refined base model plus numerical predictors

> TARGET_FLAG ~ MSTATUS + EDUCATION_Bachelors + JOB_Clerical + JOB_Manager +
                CAR_TYPE + CAR_USE + REVOKED + URBANICITY + KIDSDRIV + HOMEKIDS + 
                CLM_FREQ + INCOME + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + 
                OLDCLAIM + MVR_PTS,

Building on model 1, model 2 excludes the `RED_CAR` variable and adds in all of the numerical variables to see if they add value to our model.  After the initial model statistics were examined we then further refined the model by removing `AGE`, `CAR_AGE`, `SEX`, `YOJ`, and `PARENT1` due to low p-value significance.  `EDUCATION`, and `JOB` were only significant if the education was 'Bachelors' or if the job was 'Manager', so two new binary variables, `EDUCATION_Bachelors` and `JOB_Manager` were added to the dataset indicating yes or no for these specific education and job values.  

<!-- ```{r} -->
<!-- summary(model.3) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- drop1(mod.3) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- MASS::stepAIC(mod.3, trace=0) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- knitr::kable(vif(model.3$finalModel)) -->
<!-- ``` -->

```{r t7}
mod3_summary
```

```{r fig.cap="Model 3 ROC Curve", fig.height=3, fig.width=3.5}
plot(roc(train$TARGET_FLAG, pred.3.raw), main="ROC Curve")
```

```{r}
kable(auc(roc(train$TARGET_FLAG, pred.3.raw)), caption="Area Under the Curve")
```

### Marginal Model Plots

```{r fig.height=7, fig.width=8, fig.cap="Model 2 Marginal Model Plots"}
mmps(mod.3)
```


\newpage

### Model 3 - Binary logistic model

Model 3 takes a similar approach to model 2 by incorporating all numeric predictor variables plus the categorical predictors that were found to be singnificant in the previous model.  Skewed numeric predictors (`BLUEBOOK`, `CAR_AGE`, `HOME_VAL`, `INCOME`, `MVR_PTS`, `OLDCLAIM`, `TIF`, and `TRAVTIME`, ) were log transformed and added to the model as additional predictors.  `AGE` and `YOJ` were not included since they were already normally distributed and determined not to be significant in the previous models.  The model was then refined through backward elimination.

#### Original Model 

> TARGET_FLAG ~ MSTATUS + EDUCATION_Bachelors + JOB_Clerical + JOB_Manager + 
                CAR_TYPE + CAR_USE + REVOKED + URBANICITY + KIDSDRIV + HOMEKIDS + 
                CLM_FREQ + BLUEBOOK + CAR_AGE + HOME_VAL + INCOME + MVR_PTS + 
                OLDCLAIM + TIF + TRAVTIME + log(BLUEBOOK) + log(CAR_AGE+1) + 
                log(HOME_VAL+1) + log(INCOME+1) + log(MVR_PTS+1) + 
                log(OLDCLAIM+1) + log(TIF) + log(TRAVTIME),
                
#### Model after backward elimination process

> TARGET_FLAG ~ MSTATUS + EDUCATION_Bachelors + JOB_Clerical + 
                JOB_Manager + CAR_TYPE + CAR_USE + REVOKED + URBANICITY + 
                KIDSDRIV + HOMEKIDS + CAR_AGE + HOME_VAL + INCOME + MVR_PTS + 
                OLDCLAIM + log(BLUEBOOK) + log(INCOME+1) + 
                log(OLDCLAIM+1) + log(TIF) + log(TRAVTIME),

```{r t8}
mod4_summary
```

```{r fig.cap="Model 4 ROC Curve", fig.height=3, fig.width=3.5}
plot(roc(train$TARGET_FLAG, pred.4.raw), main="ROC Curve")
```

```{r}
kable(auc(roc(train$TARGET_FLAG, pred.4.raw)), caption="Area Under the Curve")
```

### Marginal Model Plots

```{r fig.height=7, fig.width=8, fig.cap="Model 3 Marginal Model Plots"}
mmps(mod.4, layout = c(4, 3))
```


\newpage

## Regression Model: Models 5, 6

The next two models are multiple linear regression models aimed at predicting the value of claims based on different approaches, including constraining the cases based on `TARGET_FLAG` (i.e. based on whether or not a claim was filed) and different approaches to selecting explanatory variables.

23 lines were removed where `TARGET_AMT` greater than $45,000; these lines had a `BLUEBOOK` value far less than the car crash cost. 
A new variable `mileage` was created based on `TRAVTIME` and `CAR_AGE`.

### Model 5 - Multiple linear regression model

Model 5 is a multiple linear regression model built only on cases with claims where `TARGET_FLAG` equals 1. The model is refined using stepwise elimination. From the model summary it can be observed that the Adjusted R-squared value is very low at 0.04.

```{r t9}
mod5_summary
```

```{r f15, fig.cap="Model 5 Diagnostic Plots", fig.height=6}
par(mfrow=c(2,2)); plot(mod.5)
```


\newpage

### Model 6 - Multiple linear regression model

Model 6 is a multiple linear regression model built on all cases - in other words, it relaxed the constraint that a claim was filed, and so includes `TARGET_AMT` values of 0. Forward elimination was used to refine variable selection. The Adjusted R-squared value significantly improved compared to the previous model. 

```{r t10, fig.cap="Model 6 Summary"}
mod6_summary
```

```{r f16, fig.cap="Model 6 Diagnostic Plots", fig.height=6}
par(mfrow=c(2,2)); plot(mod.6)
```


\newpage

# SELECT MODELS

```{r t13}
knitr::kable(eval_mods, caption = "Confusion Matrix Summary Statistics")
```

## Pseudo R2

There is no  $R^2$ for logistic regression to further evaluate, however, there is an alternative called $pseudo R^2$ terms that can be used for evaluation.

```{r t14}
knitr::kable(pseudo.r2, caption="Pseudo R2")
```

\newpage

# Appendix

The appendix is available as script.R file in `project4_insurance` folder.

https://github.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining


