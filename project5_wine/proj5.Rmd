---
title: "CUNY SPS DATA 621 - CTG5 - HW5"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "May 15th, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 6
        fig_height: 4
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
options(scipen=999, digits = 2)
source("./source/libs.R")
source("./source/script.R")
source("./source/script_jeremy.R")
source("./source/script_betsy.R")
source("./source/script_gabby.R")
source("./source/script_rose.R")
source("./source/script_lidiia.R")
source("./source/captioner.R")
#set.seed(123)
```

\newpage

# DATA EXPLORATION

When dining in a restaurant, a sommelier can assist you in selecting a perfect wine, even if you do not know much about wine yourself. By asking about your taste preferences, they can recommend a wine that pairs well with your meal, while complementing your likes and dislikes. But what happens when you’re a large wine manufacturer, wondering how to produce wines that will sell?  If the wine manufacturer can predict the number of cases sold based on the characteristics of a wine, then that manufacturer will be able to adjust their wine offering to maximize sales.

Our data set contains information on approximately 12,000 commercially available wines.  Most of the variables are related to the chemical properties of the wine. The response variable is the number of sample cases that were purchased by wine distribution companies after sampling the wine. These cases would be used to provide tasting samples to restaurants and wine stores around the United States and promote further sales.

```{r t1}
knitr::kable(vars, caption="Data Dictionary")
```

## Summary Statistics

Continuous quantitative and categorical variables were summarized separately for the sake of clarity.

```{r t2}
knitr::kable(train_num_stats , caption="Summary statistics for numeric variables")
```

`STARS` and `LabelAppeal` can be described as ordinal categorical variables. However, because of the numerical coding, these variables were imported as if they were quantitative.  Since they are ordinal consideration was given to treating them as numerical, but ultiately the decision was made to convert them to factors.

```{r t3}
knitr::kable(train_cat_stats, caption="Summary statistics for categorical variables")
```

### Summary Statistics Graphs

The histograms in Figure 1 shows the distribution of all 15 variables.  The distribution is more peaked than a normal bell curve for most of our varibales especially `Chlorides`, `CitricAcid`, `Density`, `FixedAcidity` , `FreeSulfurDioxide`, `pH`, `ResidualSugar`, `Sulphates`, `TotalSulfurDioxide` and `ViolatileAcidity`.  Most of those are also centered at or near zero.  `AcidIndex` has a slight right skew. 

```{r f1, fig.cap="Data Distributions", fig.height=6, fig.width=8}
hist
```

```{r f2, fig.cap="TARGET Distributions by STARS Values"}
hist.STARS
```

```{r f3, fig.cap="TARGET Distributions by LabelAppeal Values"}
hist.LabelAppeal
```

Figure 2 shows that there are a large number of outliers that need to be accounted for, except for `LabelAppeal`, `AcidIndex` and `STARS` which have limited number of variations. 

```{r f4, fig.cap = "Scaled Boxplots"}
scaled_boxplots
```

## Linearity

```{r f5, fig.cap="Scatter plot between numeric predictors and the TARGET", fig.height=7}
linearity 
```

The raw predictors fail to show linear relationship with the `TARGET` except for the `AcidIndex` and `VolativeAcidity`. The Scatter Plots show a systematic, wave-like pattern for `Density`, `FixedAcidity`, `FreeSulfurDioxide`, `TotalSulfurDioxide` and `VolativeAcidity`.

### Log Transformed Data 

```{r f6, fig.cap="Scatter plot between log transformed predictors and the log transformed TARGET filtered for rows where TARGET is greater than 0", fig.height=7}
linearity_log
```

In attempt to improve the linearity of the variables against the  `TARGET` variable, we start with a log transformation on all predictors and`TARGET` variable. As a result, the linearity of `Chlorides` and `FreeSulfurDioxide` become more apparent. 

### Box-Cox

```{r f7, fig.cap="Box-Cox Plot"}
boxcox(TARGET~., data=bc, lambda=seq(-0.2,2,by=0.1))
```

### Square Root Transformed Predictors and Log Transformed Target 

In 'Linear Models with R', Faraway suggested that the square root transformation is often appropriate for count response data. The Poisson distribution is a good model for counts, and that distribution has the property that the mean is equal to the variance thus suggesting the square root transformation. A plot of each predictor square root transformed plotted against the log transformed `TARGET`.  

```{r f8, fig.cap="Scatter plot between square root transformed predictors and the square root transformed TARGET filtered for rows where TARGET is greater than 0", fig.height=7}
linearity_root_2
```

## Missing Data

```{r f9, fig.cap="Missing data", fig.height=6, fig.width=6}
plot_missing(train)
```

A number of variables are missing observations: `STARS`, `Sulphates`, `TotalSulfurDioxide`, `Alcohol`, `FreeSulfurDioxide`, `Chlorides`, `ResidualSugar`, `pH`. For `STARS`, the number is 26.25%, but the others range between 3% and 9% of total.  Approximately 50% of the cases are missing one of these variables.

\newpage


# DATA PREPARATION

```
Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.

a. Fix missing values (maybe with a Mean or Median value)

b. Create flags to suggest if a variable was missing

c. Transform data by putting it into buckets

d. Mathematical transforms such as log or square root (or use Box-Cox)

e. Combine variables (such as ratios or adding or multiplying) to create new variables

# [JO: Consider strategies to transform negative variables - arithmetic?]

```

## Missing Values

```{r f10, fig.cap = "Difference between original and imputed data"}
#density.plot
```

[Rough explanation] About half of the cases are missing at least one of the following variables: `STARS`, `Sulphates`, `TotalSulfurDioxide`, `Alcohol`, `FreeSulfurDioxide`, `Chlorides`, `ResidualSugar`, or `pH`.  We use MICE (Multivariate Imputation by Chained Equations) to impute values these values based on ... [JO following up]

Pink and blue lines indicate close fit match in distribution of imputed values and recorded values, with the exception of STARS - the addition of STARS values has given the appearance of shifting the distribution from high to low lambda values.


```{r f11, fig.height=9, fig.width=8}
#corr.plot2
```


## Transformation / Feature Engineering

There are a large number of negative values for variables for which that is nonsensical, examples: `Alcohol`, `CitricAcid`, `FixedAcidity`, `FreeSulfurDioxide`, `ResidualSugar`, `Sulphates`, `TotalSulfurDioxide`. and `VolatileAcidity`.  The range for the Poisson and negative binomial distribution has zero as a lower bound, so we can arithmetically transform the aforementioned variables to scale the lower IQR non-outlier values from zero up and drop the sub-IQR values (now the only negative values remaining).  [JO currently function not working as intended - moving all points up by the mean - IQR * 1.5, and tossing anything less than that - so tuning calculations]

```{r f12, fig.cap="Data Distributions", fig.height=6, fig.width=8}
#hist_scaled
```

(https://www.imachordata.com/do-not-log-transform-count-data-bitches/)

Alternatively, we can explore whether more information on how measurements were made can be found to discern whether there might be some reason for negative values, and if there's a possibility of systematic data errors

We'll also test the data for over-dispersion when setting up negative binomial models.  [CODING UNDERWAY]

```{r f13, fig.width = 6, fig.height=6}
#corr.plot
```

\newpage

# BUILD MODELS

```
Using the training data set, build at least two different poisson regression models, at least two different negative binomial regression models, and at least two multiple linear regression models, using different variables (or the same variables with different transformations). Sometimes poisson and negative binomial regression models give the same results. If that is the case, comment on that. Consider changing the input variables if that occurs so that you get different models. Although not covered in class, you may also want to consider building zero-inflated poisson and negative binomial regression models. You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach such as trees, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.

Discuss the coefficients in the models, do they make sense? In this case, about the only thing you can comment on is the number of stars and the wine label appeal. However, you might comment on the coefficient and magnitude of variables and how they are similar or different from model to model. For example, you might say “pH seems to have a major positive impact in my poisson regression model, but a negative effect in my multiple linear regression model”. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.
```

# Poisson regression models

```
BR I am going to add both models under one heading...
```

# Negative binomial regression models

Negative binomial regression is an alternative when there is overdispersion (\( var(Y_i) > E(Y_i) \)).

"A Poisson distribution is parameterized by $\lambda$, which happens to be both its mean and variance. While convenient, it's not often realistic. A distribution of counts will usually have a variance that's not equal to its mean. When we see this happen with data that we assume is Poisson distributed, we say we have under- or overdispersion, depending on if the variance is smaller or larger than the mean. Performing Poisson regression on count data that exhibits this behavior results in a model that doesn't fit well."

"One approach that addresses this issue is the Negative Binomial Regresion. The negative binomial distribution describes the probabilities of the occurrence of whole numbers greater than or equal to 0. Unlike the Poisson distribution, the variance and the mean are not equivalent. This suggests it might serve as a useful approximation for modeling counts with variability different from its mean. The variance of a negative binomial distribution is a function of its mean and has an additional parameter `k` called the dispersion parameter. Say our count is a random variable Y from a negative binomial distribution, when the variance of Y is: "

$$var(Y) = \mu + \mu^2lk$$

"As the dispersion parameter gets larger and larger, the variance converges to the same value as the mean, and the negative binomial turns into a Poisson distribution."

```{r}
print(paste0("Mean of Target: ", round(mean(train_imputed$TARGET, 3))))
print(paste0("Variancea of Target: ", round(var(train_imputed$TARGET),3)))
```

It appears that there is a slight overdispersion with the variance greater than the mean as shown above.

## Negative binomial regression model 1

First negative binomial model is using all variables and selecting predictors using stepAIC.

```{r}
summ(neg.min.iqr)
```

```{r}
par(mfrow=c(2,2))
plot(neg.min.iqr)
```

It is shown that there is an issue with non-constant variance and long tails in the qqplot. The best performing dataset was the one with negative values are arithmetically scaled from lower bound of IQR*1.5 to 0, and lesser values dropped.  The negative binomial model appears to have many multiple statistically significant values. 

## Negative binomial regression model 2

```{r}
#MASS::stepAIC(neg.min.iqr, trace=0)
neg.min.iqr.updated <- update(neg.min.iqr, . ~ . -FixedAcidity -ResidualSugar -CitricAcid)
summ(neg.min.iqr.updated)
```

```{r}
list(residual.deviance           = deviance(neg.min.iqr),
     residual.degrees.of.freedom = df.residual(neg.min.iqr),
     chisq.p.value               = pchisq(deviance(neg.min.iqr), df.residual(neg.min.iqr), lower = F)
     )
```

```{r}
list(residual.deviance           = deviance(neg.min.iqr.updated),
     residual.degrees.of.freedom = df.residual(neg.min.iqr.updated),
     chisq.p.value               = pchisq(deviance(neg.min.iqr.updated), df.residual(neg.min.iqr.updated), lower = F)
     )
```

There is no significant improvement after using `stepAIC` to select the needed predictors.  The reason to use Zero Dispersion Counts model is due to an inflated number of zeros in our counts target.

# Linear regression models

## Linear regression model 1

## Linear regression model 2

\newpage

# SELECT MODELS

## Comparison of models


Assessing the fit of a count regression model is not necessarily straightforward; often we just look at residuals, which invariably contain patterns of some form due to the discrete nature of the observations, or we plot observed versus fitted values as a scatter plot.

Kleiber and Zeileis (2016) https://arxiv.org/abs/1605.01311 proposes `rootogram` as an improved approach to the assessment of fit of a count regression model. The paper is illustrated using R and the authors' countreg package.

Rootograms are calculated using the rootogram() function. You can provide the observed and expected (given the model) counts as arguments to rootogram() or, most usefully for our purposes, a fitted count model object from which the relevant values will be extracted. rootogram() knows about glm, gam, gamlss, hurdle, and zeroinfl objects at the time of writing.

Three different kinds of rootograms are discussed in the paper

* Standing,
* Hanging, and
* Suspended.

Kleiber and Zeileis (2016) recommend hanging or suspended rootograms. Which type of rootogram is produced is controlled via argument style. 

We will look at six different models, two poisson models, two negative-binomial models and an ols regression thrown in for good measure.

```{r}
knitr::kable(model_select_lt, caption="Comparison of models") 
```

Both the Poisson-Logit Hurdle Regression and the zero-inflated poisson are very close in log likelihoods and BIC's.

The Poisson-Logit Hurdle Regression provides a closer fit to the observed than does the other models. The hurdle model is a modified count model in which there are two processes, one generating the zeros and one generating the positive values. 

The Poisson-logit hurdle model is clearly the best choice here. The results for this model are given below. 


```{r}
summary(hurd.mod2)
```

## Diagnostic plots

Rootogram as an improved approach to the assessment of fit of a count regression model. Expected counts, given the model, are shown by the thick red line, and observed counts are shown as bars, which in a hanging rootogram are show hanging from the red line of expected count.

```{r}
countreg_model 
```


As a final check we can also look at the Q-Q plot of the quantile residuals in the hurdle model. These look fairly normal and show no suspicious departures from the model.

```{r}
qqrplot(hurd.mod2, main = "Q-Q plot of the Poisson-Logit Hurdle Regression")
```



## Prediction



We ran predictions on our final model and plotted the distribution next to the distribution from our target in the training data set to compare.

```{r fig15, fig.height=2, fig.cap="Predictions vs. training data"}
grid.arrange(p1.pred, p2.pred, nrow = 1)
```


```{r}
kable(summary.pred.count, caption = "TARGET Predictions")
```


\newpage

# Appendix

The appendix is available as script.R file in `project5_wine` folder.

https://github.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining

```
```