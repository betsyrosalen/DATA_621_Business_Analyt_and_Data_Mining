---
title: "CUNY SPS DATA 621 - CTG5 - Final"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "May 23rd, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 5
        fig_height: 4
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
source("./source/libs.R")
source("./source/script.R")
source("./source/captioner.R")
#set.seed(123)
```


\newpage

# PROJECT DESCRIPTION AND BACKGROUND

## Background

With nearly 18MM deaths in 2015  , cardiovascular diseases (CVD) are the leading cause of death globally and growing in the developing world .  CVD is a disease class which includes heart attacks, strokes, heart failure, coronary artery disease, arrythmia, venous thrombosis, and other conditions.  About half of all Americans (47%)  have at least one of three key risk factors for heart disease: high blood pressure, high cholesterol, and smoking.

Researchers estimate that up to 90% of heart disease deaths  could be prevented.  Typical means of detection include electrocardiograms (ECGs), stress tests, and cardiac angiograms, all of which are expensive.  Risk evaluation screenings require blood samples, which are assessed alongside risk factors like tobacco use, diet, sleep disorders, physical inactivity, air pollution, and others.

More efficient, scalable, and non-invasive means of early detection could be used to trigger medical interventions, prompt preventive care by physicians, and/or engender behavioral change on the part of those prone to or suffering from CVD.  Applying data mining techniques to CVD datasets to predict risk based on existing or easy-to-collect health data could improve healthcare outcomes and mortality rates.  

The Cleveland dataset is the most complete CVD dataset and is the most frequently used in data science experimentation.  It has a small number of observations, but this is not uncommon given the costs of experimental data and privacy risk of observational data.  

## Hypothesis



## Our approach, setup, and workflow

[We should discuss this]

\newpage

# DATA PREPARATION

## Description

[Jeremy's adding in the variable description table from our proposal doc]

## Cross-validation

[Assuming this requires a little explanation]

## Bootstrap surrogata data

[Jeremy's writing this up]

Per the synthpop packace explanation (https://cran.r-project.org/web/packages/synthpop/vignettes/synthpop.pdf):
"The basic idea of synthetic data is tovreplace some or all of the observed values by sampling from appropriate probability distributions so that the essential statistical features of the original data are preserved. The approach has been developed along similar lines to recent practical experience with multiple imputation methods although synthesis is not the same as imputation. Imputation replaces data which are missing with modelled values and adjusts the inference for the additional uncertainty due to this process. For synthesis, in the circumstances when some data are missing two approaches are possible, one being to impute missing values prior to synthesis and the other to synthesise the observed patterns of missing data without estimating the missing values.  In both cases all data to be synthesised are treated as known and they are used to create the synthetic data which are then used for inference. The data collection agency generates multiple synthetic data sets and inferences are obtained by combining the results of models fitted to each of them. The formulae for the variance of estimates from synthetic data are different from those used for imputed data."

"Our aim in writing the synthpop package (Nowok, Raab, Snoke, and Dibben 2016) for R (R Core Team 2016) is a more modest one of providing test data for users of confidential datasets. Note that currently all values of variables chosen for synthesis are replaced but this will be relaxed in future versions of the package. These test data should resemble the actual data as closely as possible, but would never be used in any final analyses. The users carry out exploratory analyses and test models on the synthetic data, but they, or perhaps staff of the data collection agencies, would use the code developed on the synthetic data to run their final analyses on the original data. This approach recognises the limitations of synthetic data produced by these methods. "

## Synthesis diagnostics

The original Cleveland dataset contains n = 303 observations over ...

```{r}
# data synthesis, including summary() and compare.syn()

```

\newpage

# BUILDING MODELS

Our literature review revealed that of the many approaches that have been taken, certain types of models stand out in terms of their performance.

[Jeremy's including grid with examples of previous work]

[See notes in literature reviewed and Kaggle projects for suggestions on variable selection and feature engineering for models (links TBC)]

[See also notes on setup and packages for decision tree, random forest, SVM, and Naive Bayes  (links TBC]

## Logistic regression


## Decision tree (CHAID or C&RT?)


## Random forest


## Support Vector Machines


## Naive Bayes


\newpage

# MODEL REVIEW AND SELECTION

## Comparison of performance between models

[sensitivity, specificity, accuracy, others metrics?]

[Between different techniques and between original dataset and synthesize dataset for each technique]


## Comparison of performance viz. other studies


\newpage

# CONCLUSIONS


\newpage

# APPENDIX


## Supplemental tables and figures


```{r t1}
knitr::kable(vars, caption="Data Dictionary")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```


```{r}
knitr::kable(orig_data_stats , caption="Summary statistics for numerical variables in the original dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
knitr::kable(data_cat_stats, caption="Summary statistics for categorical variables in the original dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
knitr::kable(data_cat_stats_b, caption="Summary statistics for binary categorical variables in the original dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
knitr::kable(syn_data_stats , caption="Summary statistics for numerical variables in the synthesised dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
knitr::kable(syn_cat_stats , caption="Summary statistics for categorical variables in the synthesised dataset")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```



```{r}
knitr::kable(syn_cat_stats_b , caption="Summary statistics for binary categorical variables in the synthesised dataset")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```

```{r f1, fig.cap="Numeric and Categorical Data Distributions as a Function of TARGET", fig.height=6, fig.width=8}
hist.num
```



```{r f2, fig.cap = "Categorical Data Distributions as a Function of TARGET", fig.height=8, fig.width=8}
bar.cat
```



```{r f3, fig.cap = "Scaled Boxplots"}
scaled.boxplots
```

```{r f4, fig.cap = "Linear relationship between each numeric predictor and the target", fig.height=6, fig.width=8}
boxplots.target
```



```{r f5, fig.cap="Missing data in the synthesised dataset", fig.height=6, fig.width=6}
plot_missing(syn_df)
```

```{r f6, fig.height=9, fig.width=8}
corr.plot2
```


## R statistical programming code





The appendix is available as script.R file in `projectFinal_heart` folder.

https://github.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining

```
```