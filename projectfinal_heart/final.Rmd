---
title: "CUNY SPS DATA 621 - CTG5 - Final"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Jones"
date: "May 25th, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 5
        fig_height: 3.5
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
setwd("~/GitHub/DATA_621_Business_Analyt_and_Data_Mining/projectfinal_heart")
source("./source/libs.R")
source("./source/script.R")
source("./source/lr_betsy_gabby.R")
source("./source/rf_rose.R")
source("./source/captioner.R")
#set.seed(123)
```


\newpage

# PROJECT DESCRIPTION AND BACKGROUND

## Background

With nearly 18MM deaths in 2015, cardiovascular diseases (CVD) are the leading cause of death globally and growing in the developing world.  CVD is a disease class which includes heart attacks, strokes, heart failure, coronary artery disease, arrhythmia, venous thrombosis, and other conditions.  About half of all Americans (47%)  have at least one of three key risk factors for heart disease: high blood pressure, high cholesterol, and smoking.

Researchers estimate that up to 90% of heart disease deaths could be prevented.  Typical means of detection include electrocardiograms (ECG's), stress tests, and cardiac angiograms, all of which are expensive.  Risk evaluation screenings require blood samples, which are assessed alongside risk factors like tobacco use, diet, sleep disorders, physical inactivity, air pollution, and others.

More efficient, scalable, and non-invasive means of early detection could be used to trigger medical interventions, prompt preventive care by physicians, and/or engender behavioral change on the part of those prone to or suffering from CVD.  Applying data mining techniques to CVD datasets to predict risk based on existing or easy-to-collect health data could improve healthcare outcomes and mortality rates.  

The Cleveland dataset is the most complete CVD dataset and is the most frequently used in data science experimentation.  It has a small number of cases (n = 303) but numerous variables including the 14 most commonly selected for analysis which were are the ones selected for this analysis as well (m = 14, including the target class).  Small numbers of observations are common with health data given the costs of collecting experimental data and the privacy risk considerations of observational data.  

## Research Question and Approach

A multitude of approaches and methodologies have been attempted by researchers with the aim of predicting the presence of heart disease (the target class) based on the 13 other variables most commonly selected from the Cleveland dataset. 

For this project, different classification techniques - including logistic regression, random forests, Naive Bayes, and Support Vector Machines models - that preceding researchers have found fruitful were evaluated, synthesized data was harnessed, and attempts were made to improve upon the models' performance.

\newpage

# DATA PREPARATION

## Original Data Description

The original data set has 13 predictor variables, 8 of which are categorical and range from 2 to 5 levels while the other 5 are numeric. The target variable is a binary categorical variable that indicates whether or not the patient has heart disease with 1 indicating presence of heart disease and 0 indicating no heart disease. There were no missing values in the data set. Descriptions of each of the variables are provided in Table 1: Data Dictionary.

```{r}
knitr::kable(vars, caption="Data Dictionary")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```

### Original Data Summary Statistics

```{r}
knitr::kable(orig_num_stats, caption="Summary statistics for numeric variables in the original data set")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```


```{r}
knitr::kable(orig_cat_stats, caption="Summary statistics for categorical variables in the original dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
```


```{r eval=FALSE}
knitr::kable(orig_cat_stats_b, caption="Summary statistics for binary categorical variables in the original dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

### Original Data Summary Statistics Graphs

```{r f1, fig.cap="Numeric Data Distributions as a Function of TARGET", fig.height=6, fig.width=8}
hist.num
```


```{r f2, fig.cap = "Categorical Data Distributions as a Function of TARGET", fig.height=8, fig.width=8}
bar.cat
```


```{r f3, fig.cap = "Scaled Boxplots for Numeric Variables"}
scaled.boxplots
```


```{r f4, fig.cap = "Linear relationship between each numeric predictor and the target", fig.height=3, fig.width=8}
boxplots.target
```


```{r f6, fig.height=9, fig.width=8}
corr.plot2
```

## Cross-validation

Cross-validation is a resampling procedure to evaluate machine learning models. The in-sample data is divided randomly into equally-sized k groups, also known as folds. This approach involves randomly dividing the in-sample data into k groups or folds, of equal size.  In this approach, the validation set is the first fold, and the remaining k-1 folds have the method fit across them. To use nested cross-validation, the outer cross-validation provides performance assessment obtainable using a method of constructing a model, including feature selection. In order to independently select the features in each fold of the outer cross-validation, the inner cross-validation is utilized. The evaluation score is retained and the model discarded to summarize the skill of the model using the evaluation scores.

One of the most popular cross validation techniques is Grid Search.  Grid search experiments are common in the literature of empirical machine learning, where they are used to optimize the hyper-parameters of learning algorithms.  It is common to perform multi-stage automated grid experiment, however, fine resolution for optimization would be computationally expensive.  Grid search experiments allocates many trials to the exploration of dimensions that may not matter and suffer from poor coverage in dimensions that are important.

On the other hand, Random search found better models in most cases and required less computational time.  It is easier to carry out and more practical in terms of statistical independence of every trial.  The experiment can be stopped at any time and can be added without adjustment of grid or committing larger experiment as every trial can be carried out asynchronously.

To minimize the computation time, Coarse to Fine strategy was implemented.  During the Coarse search phase, we use the random search technique, then, simply filter the under-performing parameter values out to run Finer search to find the best values for parameters. 

## Bootstraping 'synthetic' data

The idea of synthesizing data is to replace some or all observed values so that the statistical features of the original data are preserved. This approach can be used to anonymize data subjects, keep actual observations confidential, or comply with legal or regulatory requirements regarding identifiable information while still performing data modeling or other data-related tasks.

For our purposes, the motivation to synthesize data is to augment the size of the dataset, simulating a larger number of cases based on the original distributions of the observed data using the synthpop package in R.  In this way, we can scale the data from hundreds to thousands or tens of thousands of cases with values for each variable. This enables a wider range of potential modeling techniques, and we are curious if it will allow models that can benefit from larger samples to achieve better or more stable performance.

### Synthesis diagnostics

The original Cleveland dataset contains n = 303 observations.  The synthesized dataset is simulated based on the same probability distribution, but is 20 times larger at n= 6,060.   

```{r, fig.align="center", fig.width=6, fig.height=6, fig.cap="Data Distribution - Original vs. Synthesized"}

compare_synthoriginal$plots[[1]]
compare_synthoriginal$plots[[2]]
compare_synthoriginal$plots[[3]]
compare_synthoriginal$plots[[4]]

```

### Synthesized data summary statistics


```{r}
knitr::kable(syn_num_stats, caption="Summary statistics for numerical variables in the synthesized dataset") %>%
kable_styling(latex_options = c("striped", "hold_position"))
```


```{r}
knitr::kable(syn_cat_stats, caption="Summary statistics for categorical variables in the synthesized dataset")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```


```{r eval=FALSE}
knitr::kable(syn_cat_stats_b, caption="Summary statistics for binary categorical variables in the synthesized dataset")%>%
kable_styling(latex_options = c("striped", "hold_position"))
```


```{r f5, fig.cap="Missing data in the synthesised dataset", fig.height=6, fig.width=6}
plot_missing(syn_df)
```

\newpage

# BUILDING MODELS

Our literature review highlighted a wide array of approaches to classification taken to predict the presence of heart disease using the 13 variables most commonly selected from the Cleveland dataset.  

Shouman et al. compiled an exhaustive review of over 60 papers published between 2000 and 2016 that detail different classification modeling approaches built on heart disease datasets, include the Cleveland dataset:

```{r}
knitr::kable(studies, caption="Previous Approaches Documented by Shouman")
```

Summarizing model performance (median accuracy) by type:

```{r}
knitr::kable(study_summ, caption="Average Performance by Technique")
```

We focused on a few pieces of research in particular:

- The aim of Shouman et al.'s work is to evaluate a potential low-cost heart disease expert system risk evaluation tool leveraging non-invasive data attributes.  This is explored by evaluating the Cleveland dataset alongside another dataset from Canberra not available via the UCI machine learning repository.  When constrained to Cleveland's non-invasive data attributes, the best performance is seen with a combination of age, sex, and resting blood pressure.  This line of research also explores integrating K-means clustering with decision tree models to improve accuracy.
- Assari et al.'s  broader data-mining focus finds that SVM and Naive Bayes outperform KNN (of K=7) and Decision Tree in terms of accuracy when using 10-fold cross-validation.  Its results identify the most important features as chest pain type, exercise thallium, and coronary artery disease.
- Sabay et al.  seek to assess the application of ML techniques requiring more observations to the Cleveland dataset so improve its generalizability.  To that end, a surrogate synthetic dataset is bootstrapped using the Synthpop package in R.  Logistic Regression is found to be more accurate and stable than Random Forest and Decision Tree methods, both for the original dataset as well as a 50,000-observation surrogate.  An ANN perceptron model built on a 60,000-observation surrogate dataset achieves accuracy and recall above 95%.

We selected four modeling approaches based on our review of the literature and findings:

- Logistic Regression
- Random Forest
- Support Vector Machines
- Naive Bayes

## Logistic regression

The logistic regression model involves picking one or more variables in comparison to the target. All of the available variables were initially compared to the target variable in both the original data and the synthetic data. The result of this comparison in the original data was that the patient's sex, chest pain type, and the number of vessels colored by fluoroscopy were the most influential of the variables. In the synthetic data, the major influencers were the patient's age, sex, chest pain type, serum cholesterol, and the number of vessels colored by fluoroscopy.

Multiple models were tested for their validity for each type of data. For the original data, the first model tested was looking at the individual influence of patient's sex, chest pain type, and the number of vessels colored by fluoroscopy on the target variable. The second model tested examined the relationship between the patient's sex and chest pain type, and how both of these variables interacted with each other, and then the number of vessels colored by fluoroscopy when compared with the target variable. The third model looked at all numeric variables compared to the target, and the last model looked at all categorical (factorized) variables compared to the target.

```{r}
knitr::kable(data.frame("R2"=c(0.43, 0.44, 0.29, 0.57), "Adj. R2"=c(0.41, 0.41, 0.27, 0.54), "AIC"=c(235.6967, 237.9943, 283.0819, 182.3636), "BIC"=c(270.6273, 283.4041, 307.5334, 248.7317), row.names=c("Select 1", "Select 2", "Numeric", "Factors")))
```

Across the original data, the model with the best preliminary performance was the factorized variables. 

A similar process occurred with the synthetic data that resulted in three models instead of four. The first model compared the patient's age, sex, chest pain type, serum cholesterol, and the number of vessels colored by fluoroscopy to the target variable. The second model looked at all numeric variables compared to the target, and the last model looked at all categorical (factorized) variables compared to the target.

```{r}
knitr::kable(data.frame("R2"=c(0.40, 0.18, 0.42), "Adj. R2"=c(0.39, 0.18, 0.42), "AIC"=c(4605.077, 6090.317, 4417.274), "BIC"=c(4682.913, 6135.722, 4540.515), row.names=c("Select", "Numeric", "Factors")))
```

The synthetic model proves once again the factorized variables are more influential than the others tested.

To further single out the best model, two kinds of predictions were made for each model: predictions against the test data for the data the model was created from, and predictions against the test data for the data the model was not created from.

```{r fig.caption="Accuracy and Statistics for Each Confusion Matrix"}
knitr::kable(lr_compared)
```

The most accurate model for the original data and the synthetic data was the factorized model. The most accurate model for data not matching what the model was built on was also the factorized model. The best of the best was definitely the factorized model for the original data.

```{r fig.cap="Confusion Matrix for Factorized Model on Original Data"}
grid.table(lm_factors_pred_orig_confusion$table)
```


## Random forest

Random Forest consists of numerous decision trees that are generated based on bootstrap sampling from the in-sample data.  Subsampling reduces the variance of trees substantially and the random feature selection decorrelates them to improve the predictive accuracy and control over-fitting.
  
The bootstrap resampling of the data for training each tree increases the diversity between the trees.  Each tree is composed of a root node, branch nodes, and leaf nodes.  For each node of a tree, the optimal node splitting feature is selected from a set of features that are again randomly selected.  The final output is an ensemble of random forest trees, so that classification can be performed via majority vote.

Tuning hyperparameter values in the model significantly impact the accuracy of model performance.  The three parameters that we tune in this experiment are the number of trees, maximum depth, and the number of features to randomly select.   We use a coarse to fine search strategy for hyperparameter tuning.  Our model used normally distributed random values for parameters for a few tries. Then, we incorporate cross-validation during the training process to evaluate the results.  Upon completion, the parameters from the first pass models are used to create a refined range for parameter selection.

### Hyper parameter tuning

#### Tune using caret

The caret package in R provides an excellent facility to tune machine learning algorithm parameters. Not all machine learning algorithms are available in caret for tuning. The choice of parameters is left to the developers of the package. Only those algorithm parameters that have a large effect are available for tuning in caret. As such, only `mtry` parameter is available in caret for tuning. The reason is its effect on the final accuracy and that it must be found empirically for a dataset. The `ntree` parameter is different in that it can be as large as you like, and continues to increases the accuracy up to some point. It is less difficult or critical to tune and could be limited more by compute time available more than anything.

##### Random Search

One search strategy that we can use is to try random values within a range. This can be good if we are unsure of what the value might be and we want to overcome any biases we may have for setting the parameter (like the suggested equation above). Let us try a random search for `mtry` using caret:  We can see that the most accurate value for mtry was `r rf.random$results[[1]][1]` with an accuracy of `r rf.random$results[[2]][1]`

##### Grid Search

Grid search experiments are common in the literature of empirical machine learning, where they are used to optimize the hyper-parameters of learning algorithms.  It is common to perform multi-stage automated grid experiment, however, fine resolution for optimization would be computationally expensive.  Grid search experiments allocates many trials to the exploration of dimensions that may not matter and suffer from poor coverage in dimensions that are important.  We can see that the most accurate value for `mtry` was `r rf.gridsearch$results[[1]][1]` with accuracy of `r rf.gridsearch$results[[2]][1]`

#### Tune Using Algorithm Tools

Some algorithms provide tools for tuning the parameters of the algorithm. For example, the random forest algorithm implementation in the randomForest package provides the tuneRF() function that searches for optimal mtry values given your data.  We can see that the most accurate value for mtry was `r bestmtry[1,][[1]]` with an OOBError of `r bestmtry[1,][[2]]`  This does not really match up with what we saw in the caret repeated cross validation experiment above, Nevertheless, it is an alternate way to tune the algorithm. 

#### Craft your own parameter search

##### Tune Manually

We want to keep using caret because it provides a direct point of comparison to our previous models and because of the repeated cross validation test harness that we like as it reduces the severity of overfitting. One approach is to create many caret models for our algorithm and pass in a different parameters directly to the algorithm manually. Let’s look at an example doing this to evaluate different values for `ntree` while holding mtry constant.

```{r fig.cap = "Random Forest Hyperparameter Tuning Manual Search"}
dotplot(manual.rf)
```

We can see that the most accuracy value for ntree was perhaps 1000 with a mean accuracy of 80.38% (a lift over our very first experiment using the default mtry value). The results perhaps suggest an optimal value for ntree between 2000 and 2500. Also note, we held mtry constant at the default value. We could repeat the experiment with a possible better mtry=2 from the experiment above, or try combinations of of ntree and mtry in case they have interaction effects.

##### Extend Caret

Another approach is to create a “new” algorithm for caret to support. This is the same random forest algorithm you are using, only modified so that it supports multiple tuning of multiple parameters. A risk with this approach is that the caret native support for the algorithm has additional or fancy code wrapping it that subtly but importantly changes it’s behavior. You many need to repeat prior experiments with your custom algorithm support. We can define our own algorithm to use in caret by defining a list that contains a number of custom named elements that the caret package looks for, such as how to fit and how to predict. See below for a definition of a custom random forest algorithm for use with caret that takes both an mtry and ntree parameters. Now, let’s make use of this custom list in our call to the caret train function, and try tuning different values for ntree and mtry.

```{r fig.cap = "Random Forest Hyperparameter Tuning Custom Search"}
plot(custom.result)
```

You can see that the most accurate values for ntree and mtry were 1500 and 2 with an accuracy of 84.43%. We do perhaps see some interaction effects between the number of trees and the value of ntree. 

### Random Forest Baseline Model

we will stick to tuning two parameters, the `mtry` and the `ntree` that have the most influence on accuracy of RF model. `mtry` is the number of variables randomly sampled as candidates at each split.  `ntree` is the number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times. Let us create a baseline for comparison, using recommended default values for parameters: `mtry` floor(sqrt(ncol(x))) and `ntree` 500. 

The baseline model used total 303 samples which includes 13 predictor variables to classify two classes in target. Resampling was done using cross-validation for 10 fold, repeating 3 times. The estimated accuracy is `r rf.baseline$results[[2]]`

### Random Forest Final Model And Evaluation

The final model is developed by using generated synthetic data with the help of minority class data. Simply put, it takes the minority class data points and creates new data points which lie between any two nearest data points joined by a straight line. In order to do this, the algorithm calculates the distance between two data points in the feature space, multiplies the distance by a random number between 0 and 1 and places the new data point at this new distance from one of the data points used for distance calculation. Note the number of nearest neighbors considered for data generation is also a hyperparameter and can be changed based on requirement.


```{r}
dplyr::select(eval_mods, Sensitivity, Specificity, Precision, Recall, F1)
```


```{r fig.cap = "RF Classifier cross-validation using original data"}
# Plot cross validation verses model producers accuracy
par(mfrow=c(1,2)) 
plot(rf.org.clf.cv, type = "cv", main = "CV producers accuracy", xlab = "")
plot(rf.org.clf.cv, type = "model", main = "Model producers accuracy")
```

```{r fig.cap = "RF Classifier cross-validation OOB error using original data"}
par(mfrow=c(1,2)) 
plot(rf.org.clf.cv, type = "cv", stat = "oob", main = "CV oob error", xlab = "")
plot(rf.org.clf.cv, type = "model", stat = "oob", main = "Model oob error")
```

```{r fig.cap = "RF Classifier cross-validation using synthesized data"}
par(mfrow=c(1,2)) 
plot(rf.syn.clf.cv, type = "cv", main = "CV producers accuracy", xlab = "")
plot(rf.syn.clf.cv, type = "model", main = "Model producers accuracy")
```

```{r fig.cap = "RF Classifier cross-validation OOB using synthesized data"}
par(mfrow=c(1,2)) 
plot(rf.syn.clf.cv, type = "cv", stat = "oob", main = "CV oob error", xlab = "")
plot(rf.syn.clf.cv, type = "model", stat = "oob", main = "Model oob error")	  
```

```{r fig.cap = "RF classifier ROC curve Original(left), Synthesized(right)"}
par(mfrow=c(1,2)) 
plot(ROCRPref.org, colorize=TRUE, print.cutoffs.at = seq(0.1, by=0.1), main = "ROC curve (org)")
plot(ROCRPref.syn, colorize=TRUE, print.cutoffs.at = seq(0.1, by=0.1), main = "ROC curve (syn)")
```

The chart shows accuracy and oob error graph of RF classifiers built on original data, synthesized data on top and bottom row respectively.  It appears that the classifier built on original model shows higher OOB error despite the higher accuracy rate. This is due to classifier having `seen` the test data when building the model. 


## Support Vector Machines

Per our literature review, Assari et al found that Support Vector Machine (SVM) models provided better accuracy than Naive Bayes and Decision Tree approaches.  

SVM models are a form of supervised learning used to create discriminative large-margin classifiers, defining a decision boundary between classes based on a hyperplane.  This hyperplane maximizes the margin, or distance, between the nearest points of different labeled classes while prioritizing correct classification.  The classifier can then be used to predictively categorize unlabeled examples.  

SVM models perform well in high dimensions and can be used to produce linear as well as non-linear classification depending on the kernel function employed.  However, they are prone to overfitting and can be hard to interpret, particularly when in higher dimensions.

### Kernel Methods

Kernels are algorithms that operate in high-dimensional feature space without explicitly mapping data to the coordinates of that space.  Mathematically, this 'kernel trick' means scalar solutions to dot products can be used without computationally laborious transformations.

Practically for SVM models, if we can map the feature space to higher dimensions in which the classes of cases become separable, then a linear classifier can be used to solve non-linear problems.

As kernel values depend on the inner products of feature vectors, it's best practice to scale variables to range of [0, 1] or [-1, 1], which can be accomplished with the preProcess argument of caret's train function.

Choice of kernel function has a significant impact on the outcome of an SVM model.  While this provides flexibility it also create potential points of failure, and SVM models are very sensitive to selection of kernel parameters.

Hsu et al recommend commencing with the Guassian radial basis function (RBF) kernel, which can handle nonlinear relationships, has fewer hyperparameters, and prevents fewer numerical challenges. For this project, we evaluated both RBFs and linear kernels, setting them via the method argument of caret's train function.

### Tuning

A hard margin constraint prevents the classifier from allowing the margin to overlap with values.  This constraint can be relaxed, which reduces variance but adds bias.  By manipulating the regularization, or penalty, parameter C we can decrease (soft margin) or increase (hard margin) the weight of values within the margin on overall error.  To test the impact of different C values we employ a grid search using the expand.grid function.

The tuning parameter sigma also impacts model fit.  A smaller sigma tends to yield a local classifier with less bias and more variance, while a larger sigma value tends to yield a general classifier with more bias and less variance.

### Approach

SVM models can be computationally intensive, so we first build the model and evaluate performance based on the original dataset of 303 cases before attempting with a larger synthesized dataset.  We harness different kernel functions and tuned parameters to optimize performance based on accuracy (number of correct predictions as percentage of total cases) and kappa statistics (comparing observed and expected accuracy based on random chance, or interrater reliability).

### Models

For the first SVM model, the original dataset is preprocessed (centered and scaled) before applying an RBF kernel with cross-validation (10 folds, 5 repeats).  caret identifies the optimal model based on ROC, which is highest for sigma = .033 and C = .25.  This model yields an accuracy of .813 and a kappa statistic of .622.

We generate a second SVM model with same preprocessing, RBF kernel, and cross-validation as the first, tuning it via a grid search around the sigma and C values.  This second model is optimal on ROC at a lower sigma value (.01) and higher regularization parameter (.4), displaying more variance and a harder decision boundary, but lower accuracy (.787) and kappa (.565).

As tuning did not improve accuracy, we apply the same approach and parameters of the first model (identical preprocessing, RBF kernel, and cross-validation) to the synthesized data to create a third SVM model.  Based on ROC, caret selects the optimal model with highest sigma = .031 and C = 1.  This model performs better than both preceding on accuracy (.84) and kappa (.672).  Compared with the first model, sensitivity has declined (.765 vs. .735), meaning slightly more false negatives; but specificity has increased (.854 vs. .927), for fewer false positives.

For a contrasting approach, we also trial a linear classifier (in place of the radial used in the first three models) using the synthesized data to create a fourth SVM model.  This model achieves better accuracy (.827) and kappa statistic (.65) than the first and second models but does not outperform the third.

In summary, an SVM model using a radial kernel atop the larger synthesized dataset achieved an accuracy of .84, with a stricter penalties on false positives than false negatives.  Given the model is intended as a diagnostic health tool, the optimal balance between sensitivity and specificity could be further explored (i.e. chance of misdiagnosing a patient at higher risk vs. cost of needlessly testing a patient with lower risk).

```{r, fig.cap="Support Vector Machine Model (RBF kernel with synthesized data)"}
confmtrx_svmradial3
```


## Naive Bayes


Naive Bayes classifier assumes that the presence (or absence) of a particular feature is unrelated to the presence (or absence) of any other feature. It considers all variables to independently contribute to the probability of heart disease. In spite of their naive design and apparently oversimplified assumptions, naive Bayes classifiers often work much better in many complex real world situations. Additionally, it requires a small amount of training data to estimate the parameters. 

The most accurate model was build on synthetic data. The model looked at all categorical(factorized) variables. We removed  numeric variables `age` and  `sex` from the classifier to improve our model. Additionally, `chol` variable was converted into a categorical variable. 

With the above parameters we are able to lift our accuracy to 88%, which is reasonably high. The Naive Bayes classifier is often hard to beat in terms of CPU and memory consumption, and in certain cases its performance can be very close to more complicated and slower techniques we used previously.


```{r}
confusionMatrix_nb
```


\newpage

# MODEL REVIEW AND SELECTION

## Comparison of performance between models

The logistic regression model for the original data had an accuracy of 0.813. The random forest model took the lead with an accuracy on the original data of 0.951. It maintained its lead when the support vector machines model presented with an accuracy of 0.813, just as the logistic regression model had. Lastly, the weakest of the models was the Naive Bayes model with an accuracy of 0.787 on the original data. This exact pattern also held true for the synthetic data, except with a single blip - the support vector machines model was more efficient with the synthetic data (0.840) than it was with the original data.

## Comparison of performance viz. other studies


\newpage

# CONCLUSIONS


\newpage

# APPENDIX


## Supplemental tables and figures

```{r fig.cap = "Random Forest Hyperparameter Tuning - Random Search"}
plot(rf.random)
```

```{r fig.cap = "Random Forest Hyperparameter Tuning - Grid Search"}
plot(rf.gridsearch)
```

```{r fig.cap = "Random Forest Best mtry search"}
plot(bestmtry)
```


## R statistical programming code





The appendix is available as script.R file in `projectFinal_heart` folder.

https://github.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining

```
```
