---
title: "CUNY SPS DATA 621 - CTG5 - HW5"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "May 15th, 2019"
output:
    bookdown::pdf_document2:
        toc: true
        toc_depth: 3
        number_sections: true
        fig_width: 6
        fig_height: 4
        fig_caption: true
        includes:  
            in_header: ./source/figure_placement.tex
        highlight: haddock
        df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
chooseCRANmirror(graphics=FALSE, ind=1)
options(scipen=999, digits = 2)
source("./source/libs.R")
source("./source/script.R")
source("./source/script_jeremy.R")
source("./source/script_betsy.R")
source("./source/script_gabby.R")
source("./source/script_rose.R")
source("./source/script_lidiia.R")
source("./source/captioner.R")
set.seed(123)
```

\newpage

# DATA EXPLORATION

When dining in a restaurant, a sommelier can assist you in selecting a perfect wine, even if you do not know much about wine yourself. By asking about your taste preferences, they can recommend a wine that pairs well with your meal, while complementing your likes and dislikes. But what happens when you’re a large wine manufacturer, wondering how to produce wines that will sell?  If the wine manufacturer can predict the number of cases sold based on the characteristics of a wine, then that manufacturer will be able to adjust their wine offering to maximize sales.

Our data set contains information on approximately 12,000 commercially available wines.  Most of the variables are related to the chemical properties of the wine. The response variable is the number of sample cases that were purchased by wine distribution companies after sampling the wine. These cases would be used to provide tasting samples to restaurants and wine stores around the United States and promote further sales.

```{r t1}
knitr::kable(vars, caption="Data Dictionary")
```

## Summary Statistics

Continuous quantitative and categorical variables were summarized separately for the sake of clarity.

```{r t2}
knitr::kable(train_num_stats , caption="Summary statistics for numeric variables")
```

`STARS` and `LabelAppeal` can be described as ordinal categorical variables. However, because of the numerical coding, these variables were imported as if they were quantitative.  Since they are ordinal consideration was given to treating them as numerical, but ultiately the decision was made to convert them to factors.

```{r t3}
knitr::kable(train_cat_stats, caption="Summary statistics for categorical variables")
```

### Summary Statistics Graphs

The histograms in Figure 1 shows the distribution of all 15 variables.  The distribution is more peaked than a normal bell curve for most of our varibales especially `Chlorides`, `CitricAcid`, `Density`, `FixedAcidity` , `FreeSulfurDioxide`, `pH`, `ResidualSugar`, `Sulphates`, `TotalSulfurDioxide` and `ViolatileAcidity`.  Most of those are also centered at or near zero.  `AcidIndex` has a slight right skew. 

```{r f1, fig.cap="Data Distributions", fig.height=6, fig.width=8}
hist
```

```{r f2, fig.cap="TARGET Distributions by STARS Values"}
hist.STARS
```

```{r f3, fig.cap="TARGET Distributions by LabelAppeal Values"}
hist.LabelAppeal
```

Figure 2 shows that there are a large number of outliers that need to be accounted for, except for `LabelAppeal`, `AcidIndex` and `STARS` which have limited number of variations. 

```{r f4, fig.cap = "Scaled Boxplots"}
scaled_boxplots
```

## Linearity

```{r f5, fig.cap="Scatter plot between numeric predictors and the TARGET", fig.height=7}
linearity 
```

The raw predictors fail to show linear relationship with the `TARGET` except for the `AcidIndex` and `VolativeAcidity`. The Scatter Plots show a systematic, wave-like pattern for `Density`, `FixedAcidity`, `FreeSulfurDioxide`, `TotalSulfurDioxide` and `VolativeAcidity`.

### Log Transformed Data 

```{r f6, fig.cap="Scatter plot between log transformed predictors and the log transformed TARGET filtered for rows where TARGET is greater than 0", fig.height=7}
linearity_log
```

In attempt to improve the linearity of the variables against the  `TARGET` variable, we start with a log transformation on all predictors and`TARGET` variable. As a result, the linearity of `Chlorides` and `FreeSulfurDioxide` become more apparent. 

### Box-Cox

```{r f7, fig.cap="Box-Cox Plot"}
boxcox(TARGET~., data=bc, lambda=seq(-0.2,2,by=0.1))
```

### Square Root Transformed Predictors and Log Transformed Target 

In 'Linear Models with R', Faraway suggested that the square root transformation is often appropriate for count response data. The Poisson distribution is a good model for counts, and that distribution has the property that the mean is equal to the variance thus suggesting the square root transformation. A plot of each predictor square root transformed plotted against the log transformed `TARGET`.  

```{r f8, fig.cap="Scatter plot between square root transformed predictors and the square root transformed TARGET filtered for rows where TARGET is greater than 0", fig.height=7}
linearity_root_2
```

## Missing Data

```{r f9, fig.cap="Missing data", fig.height=6, fig.width=6}
plot_missing(train)
```

A number of variables are missing observations: `STARS`, `Sulphates`, `TotalSulfurDioxide`, `Alcohol`, `FreeSulfurDioxide`, `Chlorides`, `ResidualSugar`, `pH`. For `STARS`, the number is 26.25%, but the others range between 3% and 9% of total.  Approximately 50% of the cases are missing one of these variables.

\newpage


# DATA PREPARATION

```
Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.

a. Fix missing values (maybe with a Mean or Median value)

b. Create flags to suggest if a variable was missing

c. Transform data by putting it into buckets

d. Mathematical transforms such as log or square root (or use Box-Cox)

e. Combine variables (such as ratios or adding or multiplying) to create new variables

# [JO: Consider strategies to transform negative variables - arithmetic?]

```

## Missing Values

```{r f10, fig.cap = "Difference between original and imputed data"}
density.plot
```

There are many observations that are missing multiple pieces of data. Especially the `STARS` miss more than a quarter in both train and test dataset.  This does not appear to be missing random pattern thus imputation would not be suitable. Moreover, it appears to be a highly significant predictor.  To manage this, we simply assign 0 to NA values in `STARS`.  The follwing variables are imputed using MICE  package: `Sulphates`, `TotalSulfurDioxide`, `Alcohol`, `FreeSulfurDioxide`, `Chlorides`, `ResidualSugar`, or `pH`.

Pink and blue lines indicate close fit match in distribution of imputed values and recorded values, with the exception of STARS - the addition of STARS values has given the appearance of shifting the distribution from high to low lambda values.


```{r f11, fig.height=9, fig.width=8}
# BR I don't see this in anyone's code?  Should this be here?
#corr.plot2
```


## Transformation / Feature Engineering

There are a large number of negative values for variables for which that is nonsensical, examples: `Alcohol`, `CitricAcid`, `FixedAcidity`, `FreeSulfurDioxide`, `ResidualSugar`, `Sulphates`, `TotalSulfurDioxide`. and `VolatileAcidity`.  The range for the Poisson and negative binomial distribution has zero as a lower bound, so we can arithmetically transform the aforementioned variables to scale the lower IQR non-outlier values from zero up and drop the sub-IQR values (now the only negative values remaining).  [JO currently function not working as intended - moving all points up by the mean - IQR * 1.5, and tossing anything less than that - so tuning calculations]

```{r f12, fig.cap="Data Distributions", fig.height=6, fig.width=8}
#hist_scaled
```

(https://www.imachordata.com/do-not-log-transform-count-data-bitches/)

Alternatively, we can explore whether more information on how measurements were made can be found to discern whether there might be some reason for negative values, and if there's a possibility of systematic data errors

We'll also test the data for over-dispersion when setting up negative binomial models.

```{r f13, fig.width = 6, fig.height=6}
#corr.plot
```

\newpage

# BUILD MODELS

```
Using the training data set, build at least two different poisson regression models, at least two different negative binomial regression models, and at least two multiple linear regression models, using different variables (or the same variables with different transformations). Sometimes poisson and negative binomial regression models give the same results. If that is the case, comment on that. Consider changing the input variables if that occurs so that you get different models. Although not covered in class, you may also want to consider building zero-inflated poisson and negative binomial regression models. You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach such as trees, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.

Discuss the coefficients in the models, do they make sense? In this case, about the only thing you can comment on is the number of stars and the wine label appeal. However, you might comment on the coefficient and magnitude of variables and how they are similar or different from model to model. For example, you might say “pH seems to have a major positive impact in my poisson regression model, but a negative effect in my multiple linear regression model”. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.
```

## Poisson Regression Model

Poisson regression models were run on all 4 data preparation methods we chose using a full model approach that included all predictors in each model.  The data preparation method did not seem to have any meaningful impact on the performance of the models with all four resulting in similar AIC values although the coefficients were significantly different.

```{r t4}
knitr::kable(data.prep.AICs, caption="Comparison of different data prep methods influence on AIC")
```

```{r t5}
knitr::kable(data.prep.comparison, caption="Comparison of different data prep methods influence on model coefficients")
```

Further models were created using the data prep strategy that involved adding the minimum value plus one to all variables that had negative values in order to remove all negative and zero values from our data.  

Using the full model above as a base a `step` function was run to reduce the number of variables in the model resulting in signifcantly fewer varibles.  The drop1 function was then run to see if any more variables could be reomved and `pH` was dropped from tjhe model as a result.  

An influence plot was also run to see if there were any influential points affecting the mdoel.  Five influential points (records 3953, 4940, 8887, 10108, and 12513) were identified and removed from the model to see if they improved the fit.  The improvement was modest but noticable so they were left out of the remaining poisson models.

```{r fig.height=6, fig.width=6, fig.cap="Poisson model influence plot"}
car::influencePlot(pois.mod.2)
```

#### Base Poisson Model Statistics and Coefficients

```{r t6}
pois3.summary
```

```{r fig.height=6}
pois_plot
```


### Quasipoisson Model

Next a quasipoisson model was tried using all the same steps outlined for the poisson model above that resulted in a model that had very little change and no real improvement to the model fit over the poisson model.  The standard errors of the coefficients were moderately reduced for about half the variables but significantly increased for the other half.  

```{r t7}
pois.quasi.compare
```

#### Quasipoisson Model Statistics and Coefficients

```{r t8}
quasi.summary
```

```{r fig.height=6}
quasi_plot
```

### Hurdle and Zero-inflated Poisson Models

Hurdle and Zero-inflated models are two ways of dealing with zero-inflated data. a A plot showing the expected vs. observed counts of the `TARGET` values was created in order to see if the data might be zero-inflated.  We can clearly see in the plot that we have a much greater occurance of zero values than we would expect while the rest of the values are much closer to our expectations.  The shape of the plot does peak at 3, 4 and 5 crases and then curve downward on the right side at 1, 2, and 3 cases indicating that the observed values for the purchase of 3, 4 or 5 cases are much higher than we would expect and the observed values for the purchase of 1 or 2 cases are much lower than we would expect.

```{r fig.width=4, fig.height=4}
# Plot expected vs observed counts of TARGET to show zero inflation
ocount <- table(train_plusmin$TARGET)[1:9]
pcount <- colSums(predprob(pois.mod.3)[,1:9])
plot(pcount,ocount,type="n", main="TARGET Counts", xlab="Predicted",
     ylab="Observed", yaxt = "none") +
    text(pcount,ocount, 0:8)
axis(2, seq(0,4000,1000))
```

Plots of the two categorical variables, `STARS` and `LabelAppeal`, when the target is zero vs. when it is not zero show a sginifcant difference for `STARS` but virtually no difference for `LabelAppeal`.  The `STARS` plot strongly indicates that a hurdle or zero-inflated or possibly piecewise model may be a better fit.  While the `LabelAppeal` plot only supports the choice of hurdle and zero-inflated models.

```{r fig.height=5, fig.width=7, fig.cap="Difference in distributions for STARS when TARGET equals zero vs. greater than zero"}

par(mfrow=c(2,2))

counts <- table(train[train[, 'TARGET']==0, 'STARS'])
barplot(counts, main="TARGET equals Zero",
        xlab="STARS", col="#58BFFF")
counts <- table(train[train[, 'TARGET']>0, 'STARS'])
barplot(counts, main="TARGET does NOT equal Zero",
     xlab="STARS", col="#58BFFF")

# Plots to show that the distribution for the TARGET is very different when STARS equals zero
# Also shows zero inflation
counts <- table(train[train[, 'STARS']==0, 'TARGET'])
barplot(counts, main="STARS equals Zero",
        xlab="TARGET", col="#58BFFF")
counts <- table(train[train[, 'STARS']>0, 'TARGET'])
barplot(counts, main="STARS does NOT equal Zero",
        xlab="TARGET", col="#58BFFF")
```

```{r fig.height=5, fig.width=7, fig.cap="Difference in distributions for LabelAppeal when TARGET is less than or equal to zero vs. greater than zero"}

par(mfrow=c(2,2))

# Plots to show that the distribution for the LabelAppeal is NOT very different when TARGET equals zero
counts <- table(train[train[, 'TARGET']==0, 'LabelAppeal'])
barplot(counts, main="TARGET equals Zero",
        xlab="LabelAppeal", col="#58BFFF")
counts <- table(train[train[, 'TARGET']>0, 'LabelAppeal'])
barplot(counts, main="TARGET does NOT equal Zero",
        xlab="LabelAppeal", col="#58BFFF")

# Plots to show that the distribution for the TARGET is NOT very different when LabelAppeal is less than or greater than zero
# Also shows zero inflation
counts <- table(train[train[, 'LabelAppeal']<0, 'TARGET'])
barplot(counts, main="LabelAppeal less than Zero",
        xlab="TARGET", col="#58BFFF")
counts <- table(train[train[, 'LabelAppeal']>=0, 'TARGET'])
barplot(counts, main="LabelAppeal greater than or equal to Zero",
        xlab="TARGET", col="#58BFFF")
```

Zero-inflation is also clearly seen in the Diagnostic Distribution Plots below.

```{r}
#par(mfrow=c(2,2))
Ord_plot(train_plusmin$TARGET)
distplot(train_plusmin$TARGET, type="poisson")
distplot(train_plusmin$TARGET, type="nbinomial")
distplot(train_plusmin$TARGET, type="binomial")
```

Hurdle and zero-inflated models were both run and refined using backward elimination on the data with the influential points already removed.  Both models showed significant improvement over the poisson and quasipoisson models with significantly lower AIC values and rootogram plots that showed a closer fit and less dispersion, although dispersion is still an issue since there is still evidence of over and under dispersion in the hurdle and zero-inflated rootogram plots.

#### Rootogram Plots

```{r fig.width=7, fig.height=5}
par(mfrow=c(2,2))
countreg::rootogram(pois.mod.1)
countreg::rootogram(pois.mod.3)
countreg::rootogram(hurd.mod2)
countreg::rootogram(zi.mod2)
```

#### Hurdle Model Statistics and Coefficients

```{r t9}
hurd.summ
```

#### Zero-inflated Poisson Model Statistics and Coefficients

```{r t10}
zi.summ
```


## Negative binomial regression models

Negative binomial regression is an alternative when there is overdispersion (\( var(Y_i) > E(Y_i) \)).

"A Poisson distribution is parameterized by $\lambda$, which happens to be both its mean and variance. While convenient, it's not often realistic. A distribution of counts will usually have a variance that's not equal to its mean. When we see this happen with data that we assume is Poisson distributed, we say we have under- or overdispersion, depending on if the variance is smaller or larger than the mean. Performing Poisson regression on count data that exhibits this behavior results in a model that doesn't fit well."

"One approach that addresses this issue is the Negative Binomial Regresion. The negative binomial distribution describes the probabilities of the occurrence of whole numbers greater than or equal to 0. Unlike the Poisson distribution, the variance and the mean are not equivalent. This suggests it might serve as a useful approximation for modeling counts with variability different from its mean. The variance of a negative binomial distribution is a function of its mean and has an additional parameter `k` called the dispersion parameter. Say our count is a random variable Y from a negative binomial distribution, when the variance of Y is: "

$$var(Y) = \mu + \mu^2lk$$

"As the dispersion parameter gets larger and larger, the variance converges to the same value as the mean, and the negative binomial turns into a Poisson distribution."

```{r}
print(paste0("Mean of Target: ", round(mean(train_imputed$TARGET, 3))))
print(paste0("Variancea of Target: ", round(var(train_imputed$TARGET),3)))
```

It appears that there is a slight overdispersion with the variance greater than the mean as shown above.

### Negative binomial regression model 1

First negative binomial model is using all variables and selecting predictors using stepAIC.

```{r t11}
summ(neg.min.iqr)
```

```{r}
par(mfrow=c(2,2))
plot(neg.min.iqr)
```

It is shown that there is an issue with non-constant variance and long tails in the qqplot. The best performing dataset was the one with negative values are arithmetically scaled from lower bound of IQR*1.5 to 0, and lesser values dropped.  The negative binomial model appears to have many multiple statistically significant values. 

### Negative binomial regression model 2

```{r t12}
#MASS::stepAIC(neg.min.iqr, trace=0)
neg.min.iqr.updated <- update(neg.min.iqr, . ~ . -FixedAcidity -ResidualSugar -CitricAcid)
summ(neg.min.iqr.updated)
```

```{r}
list(residual.deviance           = deviance(neg.min.iqr),
     residual.degrees.of.freedom = df.residual(neg.min.iqr),
     chisq.p.value               = pchisq(deviance(neg.min.iqr), df.residual(neg.min.iqr), lower = F)
     )
```

```{r}
list(residual.deviance           = deviance(neg.min.iqr.updated),
     residual.degrees.of.freedom = df.residual(neg.min.iqr.updated),
     chisq.p.value               = pchisq(deviance(neg.min.iqr.updated), df.residual(neg.min.iqr.updated), lower = F)
     )
```

There is no significant improvement after using `stepAIC` to select the needed predictors.  The reason to use Zero Dispersion Counts model is due to an inflated number of zeros in our counts target.

## Linear regression models

### Linear regression model 1

### Linear regression model 2

\newpage

# SELECT MODELS

## Comparison of models

Assessing the fit of a count regression model is not necessarily straightforward; often we just look at residuals, which invariably contain patterns of some form due to the discrete nature of the observations, or we plot observed versus fitted values as a scatter plot.

Kleiber and Zeileis (2016) https://arxiv.org/abs/1605.01311 proposes `rootogram` as an improved approach to the assessment of fit of a count regression model. The paper is illustrated using R and the authors' countreg package.

Rootograms are calculated using the rootogram() function. You can provide the observed and expected (given the model) counts as arguments to rootogram() or, most usefully for our purposes, a fitted count model object from which the relevant values will be extracted. rootogram() knows about glm, gam, gamlss, hurdle, and zeroinfl objects at the time of writing.

Three different kinds of rootograms are discussed in the paper

* Standing,
* Hanging, and
* Suspended.

Kleiber and Zeileis (2016) recommend hanging or suspended rootograms. Which type of rootogram is produced is controlled via argument style. 

We will look at six different models, two poisson models, two negative-binomial models and an ols regression thrown in for good measure.

```{r t13}
knitr::kable(model_select_lt, caption="Comparison of models") 
```

Both the Poisson-Logit Hurdle Regression and the zero-inflated poisson are very close in log likelihoods and BIC's.

The Poisson-Logit Hurdle Regression provides a closer fit to the observed than does the other models. The hurdle model is a modified count model in which there are two processes, one generating the zeros and one generating the positive values. 

The Poisson-logit hurdle model is clearly the best choice here. The results for this model are given below. 


```{r t14}
summary(hurd.mod2)
```

## Diagnostic plots

Rootogram as an improved approach to the assessment of fit of a count regression model. Expected counts, given the model, are shown by the thick red line, and observed counts are shown as bars, which in a hanging rootogram are show hanging from the red line of expected count.

```{r t15}
countreg_model 
```


As a final check we can also look at the Q-Q plot of the quantile residuals in the hurdle model. These look fairly normal and show no suspicious departures from the model.

```{r }
qqrplot(hurd.mod2, main = "Q-Q plot of the Poisson-Logit Hurdle Regression")
```


## Prediction

We ran predictions on our final model and plotted the distribution next to the distribution from our target in the training data set to compare.

```{r fig15, fig.height=2, fig.cap="Predictions vs. training data"}
grid.arrange(p1.pred, p2.pred, nrow = 1)
```


```{r t16}
kable(summary.pred.count, caption = "TARGET Predictions")
```


\newpage

# Appendix

The appendix is available as script.R file in `project5_wine` folder.

https://github.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining

```
# load data
train <- read.csv ('https://raw.githubusercontent.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining/master/project5_wine/data/wine-training-data.csv',
                   stringsAsFactors = F, header = T)
test <- read.csv('https://raw.githubusercontent.com/betsyrosalen/DATA_621_Business_Analyt_and_Data_Mining/master/project5_wine/data/wine-evaluation-data.csv',
                 stringsAsFactors = F, header = T)

# remove index
train$INDEX <- NULL
test$INDEX <- NULL

vars <- rbind(c('TARGET','Number of Cases Purchased','count response'),
                 c('AcidIndex','Method of testing total acidity by using a weighted avg','continuous numerical predictor'),
                 c('Alcohol','Alcohol Content','continuous numerical predictor'),
                 c('Chlorides','Chloride content of wine','continuous numerical predictor'),
                 c('CitricAcid','Citric Acid Content','continuous numerical predictor'),
                 c('Density','Density of Wine','continuous numerical predictor'),
                 c('FixedAcidity','Fixed Acidity of Wine','continuous numerical predictor'),
                 c('FreeSulfurDioxide','Sulfur Dioxide content of wine','continuous numerical predictor'),
                 c('LabelAppeal','Marketing Score indicating the appeal of label design','categorical predictor'),
                 c('ResidualSugar','Residual Sugar of wine','continuous numerical predictor'),
                 c('STARS','Wine rating by a team of experts. 4 = Excellent, 1 = Poor','categorical predictor'),
                 c('Sulphates','Sulfate conten of wine','continuous numerical predictor'),
                 c('TotalSulfurDioxide','Total Sulfur Dioxide of Wine','continuous numerical predictor'),
                 c('VolatileAcidity','Volatile Acid content of wine','continuous numerical predictor'),
                 c('pH','pH of wine','continuous numerical predictor') )

colnames(vars) <- c('VARIABLE','DEFINITION','TYPE')


# Summary Statistics

train_num <- train[, c( 'AcidIndex','Alcohol', 'Density',
                           'Sulphates', 'pH', 'TotalSulfurDioxide','FreeSulfurDioxide', 'Chlorides',
                           'ResidualSugar', 'CitricAcid', 'VolatileAcidity','FixedAcidity', 'TARGET')]

train_cat <- train[, c('STARS', 'LabelAppeal')]
train_cat$STARS <- as.factor(train_cat$STARS )
train_cat$LabelAppeal <- as.factor(train_cat$LabelAppeal )

train_num_stats <- describe(train_num)[,c(2,8,3,5,9,4)]
train_cat_stats <- summary(train_cat[, c('STARS', 'LabelAppeal')])

# Data Distribution
hist <- train %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram(fill = "#58BFFF") +
  xlab("") +
  ylab("") +
  theme(panel.background = element_blank())

h.train <- train
h.train$STARS[is.na(h.train$STARS)] <- 0
cbPalette <- c("#58BFFF", "#3300FF", "#E69F00", "#009E73", "#CC79A7")
hist.STARS <- h.train[, c('TARGET', 'STARS')] %>%
  gather(-STARS, key = "var", value = "val") %>%
  ggplot(aes(x = val, fill=factor(STARS))) +
  geom_bar(alpha=0.8) +
  facet_wrap(~ STARS, scales = "free") +
  scale_fill_manual("STARS", values = cbPalette) +
  xlab("TARGET") +
  ylab("") +
  theme(panel.background = element_blank(), legend.position="top")
hist.STARS

hist.LabelAppeal <- h.train[, c('TARGET', 'LabelAppeal')] %>%
  gather(-LabelAppeal, key = "var", value = "val") %>%
  ggplot(aes(x = val, fill=factor(LabelAppeal))) +
  geom_bar(alpha=0.8) +
  facet_wrap(~ LabelAppeal, scales = "free") +
  scale_fill_manual("LabelAppeal", values = cbPalette) +
  xlab("TARGET") +
  ylab("") +
  theme(panel.background = element_blank(), legend.position="top")
hist.LabelAppeal
#c("#58BFFF", "#3300FF")

# Boxplot

scaled.train.num <- as.data.table(scale(train[, c('AcidIndex','Alcohol', 'Density',
                                                  'Sulphates', 'pH', 'TotalSulfurDioxide','FreeSulfurDioxide', 'Chlorides',
                                                  'ResidualSugar', 'CitricAcid', 'VolatileAcidity','FixedAcidity', 'TARGET')]))


melt.train <- melt(scaled.train.num)

scaled_boxplots <- ggplot(melt.train, aes(variable, value)) +
  geom_boxplot(width=.5, fill="#58BFFF", outlier.colour="red", outlier.size = 1) +
  stat_summary(aes(colour="mean"), fun.y=mean, geom="point",
               size=2, show.legend=TRUE) +
  stat_summary(aes(colour="median"), fun.y=median, geom="point",
               size=2, show.legend=TRUE) +
  coord_flip() +
  #scale_y_continuous(labels = scales::comma,
  #                   breaks = seq(0, 110, by = 10)) +
  labs(colour="Statistics", x="", y="") +
  scale_colour_manual(values=c("#9900FF", "#3300FF")) +
  theme(panel.background=element_blank(), legend.position="top")
scaled_boxplots

#Scatter plot between numeric predictors and the TARGET
linearity <- train %>%
  gather(-TARGET, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET)) +
  geom_point(alpha=0.1) +
  stat_smooth() +
  facet_wrap(~ var, scales = "free", ncol=3) +
  ylab("TARGET") +
  xlab("") +
  theme(panel.background = element_blank())

#Scatter plot between log transformed predictors and the log transformed TARGET filtered for rows where TARGET is greater than 0
logged_vals <- train[,c('AcidIndex','Alcohol', 'Density',
                           'Sulphates', 'pH', 'TotalSulfurDioxide','FreeSulfurDioxide', 'Chlorides',
                           'ResidualSugar', 'CitricAcid', 'VolatileAcidity','FixedAcidity', 'TARGET')]
logged_vals <- logged_vals %>%
  filter(TARGET>0) %>%
  log()

linearity_log <- logged_vals %>%
  gather(-TARGET, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET)) +
  geom_point(alpha=0.1) +
  stat_smooth() +
  facet_wrap(~ var, scales = "free", ncol=3) +
  ylab("TARGET") +
  xlab("") +
  theme(panel.background = element_blank())

#Box Cox
bc <- train[train[, 'TARGET'] > 0, ]



## Square Root Transformed Predictors and Log transformed Target Linearity Plot
X <- train[train[, 'TARGET']>0,
              c('AcidIndex','Alcohol', 'Density',
                'Sulphates', 'pH', 'TotalSulfurDioxide','FreeSulfurDioxide', 'Chlorides',
                'ResidualSugar', 'CitricAcid', 'VolatileAcidity','FixedAcidity')]
sqroot_vals <- data.table(cbind(log(train[train[, 'TARGET']>0,'TARGET']),
                                   sapply(X, sqrt)))

colnames(sqroot_vals)[1] <- 'TARGET'

linearity_root_2 <- sqroot_vals %>%
  gather(-TARGET, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET)) +
  geom_point(alpha=0.1) +
  stat_smooth() +
  facet_wrap(~ var, scales = "free", ncol=3) +
  ylab("TARGET") +
  xlab("") +
  theme(panel.background = element_blank())


# DATA PREPARATION <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# STARS NA<- 0
train$STARS[is.na(train$STARS)] <- 0
test$STARS[is.na(test$STARS)] <- 0
test$STARS <- as.factor(test$STARS)
test$LabelAppeal <- as.factor(test$LabelAppeal)

##1. Data as is : train_imputed
# Then we run imputation
# impute NAs using MICE for all variables with exception of STARS
train_mice <- mice::mice(train, m = 2, method='cart', maxit = 2, print = FALSE)
train_imputed <- mice::complete(train_mice)

train_imputed_raw <- train_imputed

train_imputed$STARS <- as.factor(train_imputed$STARS)
train_imputed$LabelAppeal <- as.factor(train_imputed$LabelAppeal)
#----------------------------------------------------------------------------------------
##2. Data by shifted by min value:

train_plusmin <- train_imputed_raw

# list of columns that will be transformed
cols <- c("FixedAcidity","VolatileAcidity",
          "CitricAcid","ResidualSugar",
          "Chlorides","FreeSulfurDioxide",
          "TotalSulfurDioxide","Sulphates","Alcohol")

# Transformation of train_plusmin by adding the minimum value plus one
for (col in cols) {
  train_plusmin[, col] <- train_plusmin[, col] + abs(min(train_plusmin[, col])) + 1
}

train_plusmin$STARS <- as.factor(train_plusmin$STARS)
train_plusmin$LabelAppeal <- as.factor(train_plusmin$LabelAppeal)
#----------------------------------------------------------------------------------------
##3. Data by Jeremy's method
# arithmetically scaled from lower bound of IQR*1.5 to 0, and lesser values dropped: train_minscaled
# Subset variables with values for frequencies / concentrations / amounts that are < 0
train_scaling_subset <- train_imputed_raw %>%
  dplyr::select(FixedAcidity,
                VolatileAcidity,
                CitricAcid,
                ResidualSugar,
                Chlorides,
                FreeSulfurDioxide,
                TotalSulfurDioxide,
                Sulphates)
# dplyr::rename_all(paste0, '_scaled')

# Function to additively scale values by amount equivalent to lower bound of 1.5 * IQR
# then drop anything below 0 and leaves NAs as they are
positive_scale <- function(x) {
  low_bound <- mean(x, na.rm = TRUE) - (stats::IQR(x, na.rm = TRUE) * .5) * 1.5
  if(is.na(x)) {
    x = NA
  } else if(x < low_bound) {
    x = 0
  } else {
    x = x + abs(low_bound)
  }
}

# Rescale subset of variables with values < 0
train_iqrscaled_subset <- lapply(train_scaling_subset,
                                 FUN = function(x) sapply(x, FUN = positive_scale)) %>%
  as.data.frame()

# Join scaled subset back to other variables
train_plusiqr15 <- train_imputed_raw %>%
  dplyr::select(TARGET,
                Density,
                pH,
                Alcohol,
                LabelAppeal,
                AcidIndex,
                STARS) %>%
  cbind(train_iqrscaled_subset)

# Rescale discrete label appeal variable and factorize
train_plusiqr15$LabelAppeal <- train_imputed_raw %>%
  select(LabelAppeal) %>%
  sapply(FUN = function(x) x + 2) %>%
  as.factor()

train_plusiqr15$STARS <- as.factor(train_plusiqr15$STARS)
#----------------------------------------------------------------------------------------
##4. Data by ABS and Log

# Convert subset of variables to absolute value
train_scaling_subset2 <- train_imputed_raw %>%
  dplyr::select(FixedAcidity,
                VolatileAcidity,
                CitricAcid,
                ResidualSugar,
                Chlorides,
                FreeSulfurDioxide,
                TotalSulfurDioxide,
                Sulphates,
                Alcohol)

train_absscaled_subset <- lapply(train_scaling_subset2,
                                 FUN = function(x) sapply(x, FUN = abs)) %>%
  as.data.frame()

# lapply(train_absscaled_subset, min)

# Join absolute value-scaled subset back to other continuous variables
train_abs <- train_imputed_raw %>%
  dplyr::select(Density,
                pH,
                AcidIndex) %>%
  cbind(train_absscaled_subset)

# Log-scale all continuous variables, adding constant of 1
train_abslog <- lapply(train_abs, FUN = function(x)
  sapply(x, FUN = function(x) log(x+1))) %>%
  as.data.frame()

# Rescale discrete label appeal variable and factorize
train_abslog$LabelAppeal <- train_imputed_raw %>%
  select(LabelAppeal) %>%
  sapply(function(x) x + 2) %>%
  as.factor()

# Map remaining variables to dataframe
#train_abslog$INDEX <- train_imputed$INDEX
train_abslog$TARGET <- train_imputed_raw$TARGET
train_abslog$STARS <- train_imputed_raw$STARS
train_abslog$STARS <- as.factor(train_abslog$STARS)

```